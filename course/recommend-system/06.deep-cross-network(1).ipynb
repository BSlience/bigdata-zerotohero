{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一步，导入 TensorFlow 包。 \n",
    "\n",
    "如果你按照实战准备一的步骤配置好了 TensorFlow Python 环境，就可以成功地导入 TensorFlow 包。接下来，你要做的就是定义好训练数据的路径 TRAIN_DATA_URL 了，然后根据你自己训练数据的本地路径，替换参考代码中的路径就可以了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bigdata/.keras/datasets/modelSamples.csv\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA_URL = \"file:///opt/bigdata/lab/recommend-system/datas/modelSamples.csv\"\n",
    "samples_file_path = tf.keras.utils.get_file(\"modelSamples.csv\", TRAIN_DATA_URL)\n",
    "print(samples_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二步是载入训练数据\n",
    "\n",
    "我们利用 TensorFlow 自带的 CSV 数据集的接口载入训练数据。注意这里有两个比较重要的参数，一个是 label_name，它指定了 CSV 数据集中的标签列。另一个是 batch_size，它指定了训练过程中，一次输入几条训练数据进行梯度下降训练。载入训练数据之后，我们把它们分割成了测试集和训练集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(file_path):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_path,\n",
    "        batch_size=12,\n",
    "        label_name='label',\n",
    "        na_value=\"?\",\n",
    "        num_epochs=1,\n",
    "        ignore_errors=True)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# sample dataset size 110830/12(batch_size) = 9235\n",
    "raw_samples_data = get_dataset(samples_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = raw_samples_data.take(1000)\n",
    "train_dataset = raw_samples_data.skip(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: (OrderedDict([(movieId, (None,)), (userId, (None,)), (rating, (None,)), (timestamp, (None,)), (releaseYear, (None,)), (movieGenre1, (None,)), (movieGenre2, (None,)), (movieGenre3, (None,)), (movieRatingCount, (None,)), (movieAvgRating, (None,)), (movieRatingStddev, (None,)), (userRatedMovie1, (None,)), (userRatedMovie2, (None,)), (userRatedMovie3, (None,)), (userRatedMovie4, (None,)), (userRatedMovie5, (None,)), (userRatingCount, (None,)), (userAvgReleaseYear, (None,)), (userReleaseYearStddev, (None,)), (userAvgRating, (None,)), (userRatingStddev, (None,)), (userGenre1, (None,)), (userGenre2, (None,)), (userGenre3, (None,)), (userGenre4, (None,)), (userGenre5, (None,))]), (None,)), types: (OrderedDict([(movieId, tf.int32), (userId, tf.int32), (rating, tf.float32), (timestamp, tf.int32), (releaseYear, tf.int32), (movieGenre1, tf.string), (movieGenre2, tf.string), (movieGenre3, tf.string), (movieRatingCount, tf.int32), (movieAvgRating, tf.float32), (movieRatingStddev, tf.float32), (userRatedMovie1, tf.int32), (userRatedMovie2, tf.int32), (userRatedMovie3, tf.int32), (userRatedMovie4, tf.int32), (userRatedMovie5, tf.int32), (userRatingCount, tf.int32), (userAvgReleaseYear, tf.int32), (userReleaseYearStddev, tf.float32), (userAvgRating, tf.float32), (userRatingStddev, tf.float32), (userGenre1, tf.string), (userGenre2, tf.string), (userGenre3, tf.string), (userGenre4, tf.string), (userGenre5, tf.string)]), tf.int32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_samples_data.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第三步是载入类别型特征。 \n",
    "\n",
    "\n",
    "我们用到的类别型特征主要有这三类，分别是 genre、userId 和 movieId。在载入 genre 类特征时，我们采用了 tf.feature_column.categorical_column_with_vocabulary_list 方法把字符串型的特征转换成了 One-hot 特征。在这个转换过程中我们需要用到一个词表，你可以看到我在开头就定义好了包含所有 genre 类别的词表 genre_vocab。\n",
    "\n",
    "在转换 userId 和 movieId 特征时，我们又使用了 tf.feature_column.categorical_column_with_identity 方法把 ID 转换成 One-hot 特征，这个方法不用词表，它会直接把 ID 值对应的那个维度置为 1。比如，我们输入这个方法的 movieId 是 340，总的 movie 数量是 1001，使用这个方法，就会把这个 1001 维的 One-hot movieId 向量的第 340 维置为 1，剩余的维度都为 0。\n",
    "\n",
    "为了把稀疏的 One-hot 特征转换成稠密的 Embedding 向量，我们还需要在 One-hot 特征外包裹一层 Embedding 层，你可以看到 tf.feature_column.embedding_column(movie_col, 10) 方法完成了这样的操作，它在把 movie one-hot 向量映射到了一个 10 维的 Embedding 层上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_vocab = ['Film-Noir', 'Action', 'Adventure', 'Horror', 'Romance', 'War', 'Comedy', 'Western', 'Documentary',\n",
    "               'Sci-Fi', 'Drama', 'Thriller',\n",
    "               'Crime', 'Fantasy', 'Animation', 'IMAX', 'Mystery', 'Children', 'Musical']\n",
    "\n",
    "\n",
    "GENRE_FEATURES = {\n",
    "    'userGenre1': genre_vocab,\n",
    "    'userGenre2': genre_vocab,\n",
    "    'userGenre3': genre_vocab,\n",
    "    'userGenre4': genre_vocab,\n",
    "    'userGenre5': genre_vocab,\n",
    "    'movieGenre1': genre_vocab,\n",
    "    'movieGenre2': genre_vocab,\n",
    "    'movieGenre3': genre_vocab\n",
    "}\n",
    "\n",
    "\n",
    "categorical_columns = []\n",
    "for feature, vocab in GENRE_FEATURES.items():\n",
    "    cat_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        key=feature, vocabulary_list=vocab)\n",
    "    emb_col = tf.feature_column.embedding_column(cat_col, 10)\n",
    "    categorical_columns.append(emb_col)\n",
    "\n",
    "\n",
    "movie_col = tf.feature_column.categorical_column_with_identity(key='movieId', num_buckets=1001)\n",
    "movie_emb_col = tf.feature_column.embedding_column(movie_col, 10)\n",
    "categorical_columns.append(movie_emb_col)\n",
    "\n",
    "\n",
    "user_col = tf.feature_column.categorical_column_with_identity(key='userId', num_buckets=30001)\n",
    "user_emb_col = tf.feature_column.embedding_column(user_col, 10)\n",
    "categorical_columns.append(user_emb_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四步是数值型特征的处理。 \n",
    "\n",
    "这一步非常简单，我们直接把特征值输入到 MLP 内，然后把特征逐个声明为 tf.feature_column.numeric_column 就可以了，不需要经过其他的特殊处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = [tf.feature_column.numeric_column('releaseYear'),\n",
    "                   tf.feature_column.numeric_column('movieRatingCount'),\n",
    "                     tf.feature_column.numeric_column('movieAvgRating'),\n",
    "                     tf.feature_column.numeric_column('movieRatingStddev'),\n",
    "                     tf.feature_column.numeric_column('userRatingCount'),\n",
    "                     tf.feature_column.numeric_column('userAvgRating'),\n",
    "                     tf.feature_column.numeric_column('userRatingStddev')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第五步是定义模型结构。 \n",
    "\n",
    "这一步的实现代码也非常简洁，我们直接利用 DenseFeatures 把类别型 Embedding 特征和数值型特征连接在一起形成稠密特征向量，然后依次经过两层 128 维的全连接层，最后通过 sigmoid 输出神经元产生最终预估值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessing_layer = tf.keras.layers.DenseFeatures(numerical_columns + categorical_columns)\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    preprocessing_layer,\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第六步是定义模型训练相关的参数。 \n",
    "\n",
    "在这一步中，我们需要设置模型的损失函数，梯度反向传播的优化方法，以及模型评估所用的指标。关于损失函数，我们使用的是二分类问题最常用的二分类交叉熵，优化方法使用的是深度学习中很流行的 adam，最后是评估指标，使用了准确度 accuracy 作为模型评估的指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第七步是模型的训练和评估。 \n",
    "\n",
    "TensorFlow 模型的训练过程和 Spark MLlib 一样，都是调用 fit 函数，然后使用 evaluate 函数在测试集上进行评估。不过，这里我们要注意一个参数 epochs，它代表了模型训练的轮数，一轮代表着使用所有训练数据训练一遍，epochs=10 代表着训练 10 遍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'collections.OrderedDict'> input: OrderedDict([('movieId', <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=int32>), ('userId', <tf.Tensor 'ExpandDims_17:0' shape=(None, 1) dtype=int32>), ('rating', <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=float32>), ('timestamp', <tf.Tensor 'ExpandDims_9:0' shape=(None, 1) dtype=int32>), ('releaseYear', <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=int32>), ('movieGenre1', <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=string>), ('movieGenre2', <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=string>), ('movieGenre3', <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=string>), ('movieRatingCount', <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=int32>), ('movieAvgRating', <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float32>), ('movieRatingStddev', <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=float32>), ('userRatedMovie1', <tf.Tensor 'ExpandDims_18:0' shape=(None, 1) dtype=int32>), ('userRatedMovie2', <tf.Tensor 'ExpandDims_19:0' shape=(None, 1) dtype=int32>), ('userRatedMovie3', <tf.Tensor 'ExpandDims_20:0' shape=(None, 1) dtype=int32>), ('userRatedMovie4', <tf.Tensor 'ExpandDims_21:0' shape=(None, 1) dtype=int32>), ('userRatedMovie5', <tf.Tensor 'ExpandDims_22:0' shape=(None, 1) dtype=int32>), ('userRatingCount', <tf.Tensor 'ExpandDims_23:0' shape=(None, 1) dtype=int32>), ('userAvgReleaseYear', <tf.Tensor 'ExpandDims_11:0' shape=(None, 1) dtype=int32>), ('userReleaseYearStddev', <tf.Tensor 'ExpandDims_25:0' shape=(None, 1) dtype=float32>), ('userAvgRating', <tf.Tensor 'ExpandDims_10:0' shape=(None, 1) dtype=float32>), ('userRatingStddev', <tf.Tensor 'ExpandDims_24:0' shape=(None, 1) dtype=float32>), ('userGenre1', <tf.Tensor 'ExpandDims_12:0' shape=(None, 1) dtype=string>), ('userGenre2', <tf.Tensor 'ExpandDims_13:0' shape=(None, 1) dtype=string>), ('userGenre3', <tf.Tensor 'ExpandDims_14:0' shape=(None, 1) dtype=string>), ('userGenre4', <tf.Tensor 'ExpandDims_15:0' shape=(None, 1) dtype=string>), ('userGenre5', <tf.Tensor 'ExpandDims_16:0' shape=(None, 1) dtype=string>)])\n",
      "Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'collections.OrderedDict'> input: OrderedDict([('movieId', <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=int32>), ('userId', <tf.Tensor 'ExpandDims_17:0' shape=(None, 1) dtype=int32>), ('rating', <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=float32>), ('timestamp', <tf.Tensor 'ExpandDims_9:0' shape=(None, 1) dtype=int32>), ('releaseYear', <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=int32>), ('movieGenre1', <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=string>), ('movieGenre2', <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=string>), ('movieGenre3', <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=string>), ('movieRatingCount', <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=int32>), ('movieAvgRating', <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float32>), ('movieRatingStddev', <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=float32>), ('userRatedMovie1', <tf.Tensor 'ExpandDims_18:0' shape=(None, 1) dtype=int32>), ('userRatedMovie2', <tf.Tensor 'ExpandDims_19:0' shape=(None, 1) dtype=int32>), ('userRatedMovie3', <tf.Tensor 'ExpandDims_20:0' shape=(None, 1) dtype=int32>), ('userRatedMovie4', <tf.Tensor 'ExpandDims_21:0' shape=(None, 1) dtype=int32>), ('userRatedMovie5', <tf.Tensor 'ExpandDims_22:0' shape=(None, 1) dtype=int32>), ('userRatingCount', <tf.Tensor 'ExpandDims_23:0' shape=(None, 1) dtype=int32>), ('userAvgReleaseYear', <tf.Tensor 'ExpandDims_11:0' shape=(None, 1) dtype=int32>), ('userReleaseYearStddev', <tf.Tensor 'ExpandDims_25:0' shape=(None, 1) dtype=float32>), ('userAvgRating', <tf.Tensor 'ExpandDims_10:0' shape=(None, 1) dtype=float32>), ('userRatingStddev', <tf.Tensor 'ExpandDims_24:0' shape=(None, 1) dtype=float32>), ('userGenre1', <tf.Tensor 'ExpandDims_12:0' shape=(None, 1) dtype=string>), ('userGenre2', <tf.Tensor 'ExpandDims_13:0' shape=(None, 1) dtype=string>), ('userGenre3', <tf.Tensor 'ExpandDims_14:0' shape=(None, 1) dtype=string>), ('userGenre4', <tf.Tensor 'ExpandDims_15:0' shape=(None, 1) dtype=string>), ('userGenre5', <tf.Tensor 'ExpandDims_16:0' shape=(None, 1) dtype=string>)])\n",
      "Consider rewriting this model with the Functional API.\n",
      "8232/8232 [==============================] - 53s 6ms/step - loss: 6.5381 - accuracy: 0.5514\n",
      "Epoch 2/10\n",
      "8232/8232 [==============================] - 49s 6ms/step - loss: 0.7116 - accuracy: 0.6428\n",
      "Epoch 3/10\n",
      "8232/8232 [==============================] - 49s 6ms/step - loss: 0.5638 - accuracy: 0.7134\n",
      "Epoch 4/10\n",
      "8232/8232 [==============================] - 49s 6ms/step - loss: 0.5353 - accuracy: 0.7335\n",
      "Epoch 5/10\n",
      "8232/8232 [==============================] - 49s 6ms/step - loss: 0.5118 - accuracy: 0.7491\n",
      "Epoch 6/10\n",
      "8232/8232 [==============================] - 49s 6ms/step - loss: 0.5048 - accuracy: 0.7513\n",
      "Epoch 7/10\n",
      "8232/8232 [==============================] - 49s 6ms/step - loss: 0.4919 - accuracy: 0.7585\n",
      "Epoch 8/10\n",
      "8232/8232 [==============================] - 51s 6ms/step - loss: 0.4847 - accuracy: 0.7640\n",
      "Epoch 9/10\n",
      "8232/8232 [==============================] - 50s 6ms/step - loss: 0.4788 - accuracy: 0.7681\n",
      "Epoch 10/10\n",
      "8232/8232 [==============================] - 49s 6ms/step - loss: 0.4724 - accuracy: 0.7702\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f75e79b8340>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "\n",
    "print('\\n\\nTest Loss {}, Test Accuracy {}'.format(test_loss, test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
