{
  "paragraphs": [
    {
      "text": "%md\n# Overview\nSpark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams.\n\n![](http://spark.apache.org/docs/latest/img/streaming-arch.png)\n\nInternally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.\n\n![](http://spark.apache.org/docs/latest/img/streaming-flow.png)\n\nSpark Streaming provides a high-level abstraction called discretized stream or DStream, which represents a continuous stream of data. DStreams can be created either from input data streams from sources such as Kafka, and Kinesis, or by applying high-level operations on other DStreams. Internally, a DStream is represented as a sequence of RDDs.\n\nThis guide shows you how to start writing Spark Streaming programs with DStreams. You can write Spark Streaming programs in Scala, Java or Python (introduced in Spark 1.2), all of which are presented in this guide. You will find tabs throughout this guide that let you choose between code snippets of different languages.\n\nNote: There are a few APIs that are either different or not available in Python. Throughout this guide, you will find the tag Python API highlighting these differences.\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-11-11 01:42:23.883",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003eOverview\u003c/h1\u003e\n\u003cp\u003eSpark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams.\u003c/p\u003e\n\u003cp\u003e\u003cimg src\u003d\"http://spark.apache.org/docs/latest/img/streaming-arch.png\" /\u003e\u003c/p\u003e\n\u003cp\u003eInternally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.\u003c/p\u003e\n\u003cp\u003e\u003cimg src\u003d\"http://spark.apache.org/docs/latest/img/streaming-flow.png\" /\u003e\u003c/p\u003e\n\u003cp\u003eSpark Streaming provides a high-level abstraction called discretized stream or DStream, which represents a continuous stream of data. DStreams can be created either from input data streams from sources such as Kafka, and Kinesis, or by applying high-level operations on other DStreams. Internally, a DStream is represented as a sequence of RDDs.\u003c/p\u003e\n\u003cp\u003eThis guide shows you how to start writing Spark Streaming programs with DStreams. You can write Spark Streaming programs in Scala, Java or Python (introduced in Spark 1.2), all of which are presented in this guide. You will find tabs throughout this guide that let you choose between code snippets of different languages.\u003c/p\u003e\n\u003cp\u003eNote: There are a few APIs that are either different or not available in Python. Throughout this guide, you will find the tag Python API highlighting these differences.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1636594943883_1123921534",
      "id": "20211111-014223_1636657607",
      "dateCreated": "2021-11-11 01:42:23.883",
      "status": "READY"
    },
    {
      "text": "%md\n## A Quick Example\nBefore we go into the details of how to write your own Spark Streaming program, let’s take a quick look at what a simple Spark Streaming program looks like. Let’s say we want to count the number of words in text data received from a data server listening on a TCP socket. All you need to do is as follows.",
      "user": "anonymous",
      "dateUpdated": "2021-11-11 01:42:23.883",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eA Quick Example\u003c/h2\u003e\n\u003cp\u003eBefore we go into the details of how to write your own Spark Streaming program, let’s take a quick look at what a simple Spark Streaming program looks like. Let’s say we want to count the number of words in text data received from a data server listening on a TCP socket. All you need to do is as follows.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1636594943883_518556645",
      "id": "20211111-014223_1151791323",
      "dateCreated": "2021-11-11 01:42:23.883",
      "status": "READY"
    },
    {
      "text": "%md\nFirst, we import StreamingContext, which is the main entry point for all streaming functionality. We create a local StreamingContext with two execution threads, and batch interval of 1 second.",
      "user": "anonymous",
      "dateUpdated": "2021-11-11 01:42:23.883",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eFirst, we import StreamingContext, which is the main entry point for all streaming functionality. We create a local StreamingContext with two execution threads, and batch interval of 1 second.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1636594943883_1250503992",
      "id": "20211111-014223_1375166180",
      "dateCreated": "2021-11-11 01:42:23.883",
      "status": "READY"
    },
    {
      "text": "%pyspark\n# import pyspark\n# import findspark\n# from pyspark import SparkContext\nfrom pyspark.streaming import StreamingContext\n# findspark.init()",
      "user": "anonymous",
      "dateUpdated": "2021-11-12 11:02:39.010",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1636594943884_1126342112",
      "id": "20211111-014223_1348256137",
      "dateCreated": "2021-11-11 01:42:23.884",
      "dateStarted": "2021-11-12 11:02:39.018",
      "dateFinished": "2021-11-12 11:02:39.028",
      "status": "FINISHED"
    },
    {
      "text": "%pyspark\n# Create a local StreamingContext with two working thread and batch interval of 1 second\n# sc \u003d SparkContext(\"local[2]\", \"NetworkWordCount\")\nssc \u003d StreamingContext(sc, 1)",
      "user": "anonymous",
      "dateUpdated": "2021-11-12 11:02:44.104",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1636594943884_1289290490",
      "id": "20211111-014223_117201517",
      "dateCreated": "2021-11-11 01:42:23.884",
      "dateStarted": "2021-11-12 11:02:44.110",
      "dateFinished": "2021-11-12 11:02:44.130",
      "status": "FINISHED"
    },
    {
      "text": "%md\nUsing this context, we can create a DStream that represents streaming data from a TCP source, specified as hostname (e.g. localhost) and port (e.g. 9999).",
      "user": "anonymous",
      "dateUpdated": "2021-11-11 01:42:23.884",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eUsing this context, we can create a DStream that represents streaming data from a TCP source, specified as hostname (e.g. localhost) and port (e.g. 9999).\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1636594943884_1926177876",
      "id": "20211111-014223_1312767583",
      "dateCreated": "2021-11-11 01:42:23.884",
      "status": "READY"
    },
    {
      "text": "%pyspark\n# Create a DStream that will connect to hostname:port, like localhost:9999\nlines \u003d ssc.socketTextStream(\"localhost\", 9999)",
      "user": "anonymous",
      "dateUpdated": "2021-11-12 11:02:49.610",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1636594943884_292116551",
      "id": "20211111-014223_626284802",
      "dateCreated": "2021-11-11 01:42:23.884",
      "dateStarted": "2021-11-12 11:02:49.614",
      "dateFinished": "2021-11-12 11:02:49.655",
      "status": "FINISHED"
    },
    {
      "text": "%md\nThis lines DStream represents the stream of data that will be received from the data server. Each record in this DStream is a line of text. Next, we want to split the lines by space into words.",
      "user": "anonymous",
      "dateUpdated": "2021-11-11 01:42:23.884",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eThis lines DStream represents the stream of data that will be received from the data server. Each record in this DStream is a line of text. Next, we want to split the lines by space into words.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1636594943884_1685465780",
      "id": "20211111-014223_330470568",
      "dateCreated": "2021-11-11 01:42:23.884",
      "status": "READY"
    },
    {
      "text": "%pyspark\n# Split each line into words\nwords \u003d lines.flatMap(lambda line: line.split(\" \"))",
      "user": "anonymous",
      "dateUpdated": "2021-11-12 11:02:56.504",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1636594943884_1552054606",
      "id": "20211111-014223_1017645348",
      "dateCreated": "2021-11-11 01:42:23.884",
      "dateStarted": "2021-11-12 11:02:56.508",
      "dateFinished": "2021-11-12 11:02:56.519",
      "status": "FINISHED"
    },
    {
      "text": "%md\nflatMap is a one-to-many DStream operation that creates a new DStream by generating multiple new records from each record in the source DStream. In this case, each line will be split into multiple words and the stream of words is represented as the words DStream. Next, we want to count these words.",
      "user": "anonymous",
      "dateUpdated": "2021-11-11 01:42:23.884",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eflatMap is a one-to-many DStream operation that creates a new DStream by generating multiple new records from each record in the source DStream. In this case, each line will be split into multiple words and the stream of words is represented as the words DStream. Next, we want to count these words.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1636594943884_1286116822",
      "id": "20211111-014223_1821211355",
      "dateCreated": "2021-11-11 01:42:23.884",
      "status": "READY"
    },
    {
      "text": "%pyspark\n# Count each word in each batch\npairs \u003d words.map(lambda word: (word, 1))\nwordCounts \u003d pairs.reduceByKey(lambda x, y: x + y)\n\n# Print the first ten elements of each RDD generated in this DStream to the console\nwordCounts.pprint()",
      "user": "anonymous",
      "dateUpdated": "2021-11-12 11:03:02.914",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1636594943884_581181398",
      "id": "20211111-014223_1390697513",
      "dateCreated": "2021-11-11 01:42:23.884",
      "dateStarted": "2021-11-12 11:03:02.917",
      "dateFinished": "2021-11-12 11:03:02.949",
      "status": "FINISHED"
    },
    {
      "text": "%md\nThe words DStream is further mapped (one-to-one transformation) to a DStream of (word, 1) pairs, which is then reduced to get the frequency of words in each batch of data. Finally, wordCounts.pprint() will print a few of the counts generated every second.\n\nNote that when these lines are executed, Spark Streaming only sets up the computation it will perform when it is started, and no real processing has started yet. To start the processing after all the transformations have been setup, we finally call",
      "user": "anonymous",
      "dateUpdated": "2021-11-11 01:42:23.884",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eThe words DStream is further mapped (one-to-one transformation) to a DStream of (word, 1) pairs, which is then reduced to get the frequency of words in each batch of data. Finally, wordCounts.pprint() will print a few of the counts generated every second.\u003c/p\u003e\n\u003cp\u003eNote that when these lines are executed, Spark Streaming only sets up the computation it will perform when it is started, and no real processing has started yet. To start the processing after all the transformations have been setup, we finally call\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1636594943884_1399213249",
      "id": "20211111-014223_1858923275",
      "dateCreated": "2021-11-11 01:42:23.884",
      "status": "READY"
    },
    {
      "text": "%pyspark\nssc.start()             # Start the computation\nssc.awaitTermination()  # Wait for the computation to terminate",
      "user": "anonymous",
      "dateUpdated": "2021-11-12 11:03:13.586",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Py4JJavaError: An error occurred while calling o436.awaitTermination.\n: java.net.ConnectException: Connection refused (Connection refused)\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:607)\n\tat java.net.Socket.connect(Socket.java:556)\n\tat java.net.Socket.\u003cinit\u003e(Socket.java:452)\n\tat java.net.Socket.\u003cinit\u003e(Socket.java:262)\n\tat javax.net.DefaultSocketFactory.createSocket(SocketFactory.java:277)\n\tat py4j.CallbackConnection.start(CallbackConnection.java:226)\n\tat py4j.CallbackClient.getConnection(CallbackClient.java:238)\n\tat py4j.CallbackClient.getConnectionLock(CallbackClient.java:250)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:377)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n\tat com.sun.proxy.$Proxy30.call(Unknown Source)\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n\tat org.apache.spark.streaming.api.python.PythonTransformedDStream.compute(PythonDStream.scala:246)\n\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n\tat org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)\n\tat org.apache.spark.streaming.DStreamGraph.$anonfun$generateJobs$2(DStreamGraph.scala:123)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)\n\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n\tat org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:122)\n\tat org.apache.spark.streaming.scheduler.JobGenerator.$anonfun$generateJobs$1(JobGenerator.scala:252)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:250)\n\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:186)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:91)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:90)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\n(\u003cclass \u0027py4j.protocol.Py4JJavaError\u0027\u003e, Py4JJavaError(\u0027An error occurred while calling o436.awaitTermination.\\n\u0027, JavaObject id\u003do460), \u003ctraceback object at 0x7fad8221da50\u003e)"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1636594943884_1940000502",
      "id": "20211111-014223_1607670935",
      "dateCreated": "2021-11-11 01:42:23.884",
      "dateStarted": "2021-11-12 11:03:13.592",
      "dateFinished": "2021-11-12 11:03:16.076",
      "status": "ERROR"
    },
    {
      "text": "%md\nThe complete code can be found in the Spark Streaming example NetworkWordCount.\n\nIf you have already downloaded and built Spark, you can run this example as follows. You will first need to run Netcat (a small utility found in most Unix-like systems) as a data server by using",
      "user": "anonymous",
      "dateUpdated": "2021-11-11 01:42:23.884",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eThe complete code can be found in the Spark Streaming example NetworkWordCount.\u003c/p\u003e\n\u003cp\u003eIf you have already downloaded and built Spark, you can run this example as follows. You will first need to run Netcat (a small utility found in most Unix-like systems) as a data server by using\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1636594943884_859345419",
      "id": "20211111-014223_989316797",
      "dateCreated": "2021-11-11 01:42:23.884",
      "status": "READY"
    },
    {
      "text": "%md\n```bash\nnc -lk 9999\n```",
      "user": "anonymous",
      "dateUpdated": "2021-11-12 11:07:46.735",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cpre\u003e\u003ccode class\u003d\"language-bash\"\u003enc -lk 9999\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1636594943884_619400813",
      "id": "20211111-014223_188489403",
      "dateCreated": "2021-11-11 01:42:23.884",
      "dateStarted": "2021-11-12 11:07:46.737",
      "dateFinished": "2021-11-12 11:07:48.829",
      "status": "FINISHED"
    },
    {
      "text": "%md\n## Initializing StreamingContext\n\nTo initialize a Spark Streaming program, a StreamingContext object has to be created which is the main entry point of all Spark Streaming functionality.",
      "user": "anonymous",
      "dateUpdated": "2021-11-11 01:42:23.884",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eInitializing StreamingContext\u003c/h2\u003e\n\u003cp\u003eTo initialize a Spark Streaming program, a StreamingContext object has to be created which is the main entry point of all Spark Streaming functionality.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1636594943884_786629130",
      "id": "20211111-014223_1645457790",
      "dateCreated": "2021-11-11 01:42:23.884",
      "status": "READY"
    },
    {
      "text": "%md\nA StreamingContext object can be created from a SparkContext object.",
      "user": "anonymous",
      "dateUpdated": "2021-11-11 01:42:23.884",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eA StreamingContext object can be created from a SparkContext object.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1636594943884_225426634",
      "id": "20211111-014223_627260943",
      "dateCreated": "2021-11-11 01:42:23.884",
      "status": "READY"
    },
    {
      "text": "%pyspark\nfrom pyspark import SparkContext\nfrom pyspark.streaming import StreamingContext\n\nsc \u003d SparkContext(\"local[*]\", \"demo\")\nssc \u003d StreamingContext(sc, 1)",
      "user": "anonymous",
      "dateUpdated": "2021-11-12 11:07:53.319",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Fail to execute line 5: sc \u003d SparkContext(\"local[*]\", \"demo\")\nTraceback (most recent call last):\n  File \"/tmp/1636706740892-0/zeppelin_python.py\", line 153, in \u003cmodule\u003e\n    exec(code, _zcUserQueryNameSpace)\n  File \"\u003cstdin\u003e\", line 5, in \u003cmodule\u003e\n  File \"/spark/python/pyspark/context.py\", line 133, in __init__\n    SparkContext._ensure_initialized(self, gateway\u003dgateway, conf\u003dconf)\n  File \"/spark/python/pyspark/context.py\", line 343, in _ensure_initialized\n    callsite.function, callsite.file, callsite.linenum))\nValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app\u003dspark-shared_process, master\u003dspark://spark-master:7077) created by __init__ at \u003cstdin\u003e:46 \n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1636594943884_599823761",
      "id": "20211111-014223_564512304",
      "dateCreated": "2021-11-11 01:42:23.884",
      "dateStarted": "2021-11-12 11:07:53.324",
      "dateFinished": "2021-11-12 11:07:53.338",
      "status": "ERROR"
    },
    {
      "text": "%md\n**Points to remember:**\n\nOnce a context has been started, no new streaming computations can be set up or added to it.\nOnce a context has been stopped, it cannot be restarted.\nOnly one StreamingContext can be active in a JVM at the same time.\nstop() on StreamingContext also stops the SparkContext. To stop only the StreamingContext, set the optional parameter of stop() called stopSparkContext to false.\nA SparkContext can be re-used to create multiple StreamingContexts, as long as the previous StreamingContext is stopped (without stopping the SparkContext) before the next StreamingContext is created.",
      "user": "anonymous",
      "dateUpdated": "2021-11-11 01:42:23.884",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003e\u003cstrong\u003ePoints to remember:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eOnce a context has been started, no new streaming computations can be set up or added to it.\u003cbr/\u003eOnce a context has been stopped, it cannot be restarted.\u003cbr/\u003eOnly one StreamingContext can be active in a JVM at the same time.\u003cbr/\u003estop() on StreamingContext also stops the SparkContext. To stop only the StreamingContext, set the optional parameter of stop() called stopSparkContext to false.\u003cbr/\u003eA SparkContext can be re-used to create multiple StreamingContexts, as long as the previous StreamingContext is stopped (without stopping the SparkContext) before the next StreamingContext is created.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1636594943884_1014101610",
      "id": "20211111-014223_1250418041",
      "dateCreated": "2021-11-11 01:42:23.884",
      "status": "READY"
    },
    {
      "text": "%md\n## Discretized Streams (DStreams)\nDiscretized Stream or DStream is the basic abstraction provided by Spark Streaming. It represents a continuous stream of data, either the input data stream received from source, or the processed data stream generated by transforming the input stream. Internally, a DStream is represented by a continuous series of RDDs, which is Spark’s abstraction of an immutable, distributed dataset (see Spark Programming Guide for more details). Each RDD in a DStream contains data from a certain interval, as shown in the following figure.\n\n![](http://spark.apache.org/docs/latest/img/streaming-dstream.png)\n\nAny operation applied on a DStream translates to operations on the underlying RDDs. For example, in the earlier example of converting a stream of lines to words, the flatMap operation is applied on each RDD in the lines DStream to generate the RDDs of the words DStream. This is shown in the following figure.\n\n![](http://spark.apache.org/docs/latest/img/streaming-dstream-ops.png)\n\nThese underlying RDD transformations are computed by the Spark engine. The DStream operations hide most of these details and provide the developer with a higher-level API for convenience. These operations are discussed in detail in later sections.",
      "user": "anonymous",
      "dateUpdated": "2021-11-11 01:42:23.884",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eDiscretized Streams (DStreams)\u003c/h2\u003e\n\u003cp\u003eDiscretized Stream or DStream is the basic abstraction provided by Spark Streaming. It represents a continuous stream of data, either the input data stream received from source, or the processed data stream generated by transforming the input stream. Internally, a DStream is represented by a continuous series of RDDs, which is Spark’s abstraction of an immutable, distributed dataset (see Spark Programming Guide for more details). Each RDD in a DStream contains data from a certain interval, as shown in the following figure.\u003c/p\u003e\n\u003cp\u003e\u003cimg src\u003d\"http://spark.apache.org/docs/latest/img/streaming-dstream.png\" /\u003e\u003c/p\u003e\n\u003cp\u003eAny operation applied on a DStream translates to operations on the underlying RDDs. For example, in the earlier example of converting a stream of lines to words, the flatMap operation is applied on each RDD in the lines DStream to generate the RDDs of the words DStream. This is shown in the following figure.\u003c/p\u003e\n\u003cp\u003e\u003cimg src\u003d\"http://spark.apache.org/docs/latest/img/streaming-dstream-ops.png\" /\u003e\u003c/p\u003e\n\u003cp\u003eThese underlying RDD transformations are computed by the Spark engine. The DStream operations hide most of these details and provide the developer with a higher-level API for convenience. These operations are discussed in detail in later sections.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1636594943884_1901960765",
      "id": "20211111-014223_895487254",
      "dateCreated": "2021-11-11 01:42:23.884",
      "status": "READY"
    },
    {
      "text": "%md\n",
      "user": "anonymous",
      "dateUpdated": "2021-11-11 01:42:23.884",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1636594943884_326303508",
      "id": "20211111-014223_382358417",
      "dateCreated": "2021-11-11 01:42:23.884",
      "status": "READY"
    }
  ],
  "name": "13.SparkStreaming",
  "id": "2GKA3FE25",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}