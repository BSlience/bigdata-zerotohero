{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhM8O11sguRh"
   },
   "source": [
    "<div align=\"center\"><img src=\"asset/1613718500797.jpg\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMerdRd-guRo"
   },
   "source": [
    "# 参考文档\n",
    "- [Spark Documentation](http://spark.apache.org/docs/latest/)\n",
    "- [Spark Programming Guide](http://spark.apache.org/docs/latest/programming-guide.html)\n",
    "- [Pyspark](https://github.com/jupyter/docker-stacks)\n",
    "- [Conda](https://docs.conda.io/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 安装实验环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安装 Spark\n",
    "\n",
    "下载 Spark 压缩包，[官方下载地址](http://spark.apache.org/downloads.html)，如果你下载不下来，可以使用我下载的版本。\n",
    "\n",
    "链接: https://pan.baidu.com/s/1eTCybrBq-iWPeYMAhl1vHg  密码: 8peu\n",
    "\n",
    "我们这里选用 Spark 3.0 的版本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安装 PySpark\n",
    "\n",
    "你可以使用以下命令来安装 pyspark。\n",
    "\n",
    "```\n",
    "pip install pyspark\n",
    "pip install findspark\n",
    "```\n",
    "\n",
    "或者\n",
    "\n",
    "```\n",
    "conda install -c conda-forge pyspark \n",
    "conda install findspark\n",
    "```\n",
    "\n",
    "PySpark 的官方参考文档在[这里](https://spark.apache.org/docs/latest/api/python/),如果安装过程中出现什么问题，可以来这里找找。\n",
    "\n",
    "运行下面的命令，看看你是否已经安装成功了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aj7GtHUUguRp"
   },
   "source": [
    "## Spark 是什么？\n",
    "\n",
    "![1613718565213.jpg](asset/1613718565213.jpg)\n",
    "\n",
    "Spark 是一种基于内存的快速、通用、可扩展的大数据分析计算引擎。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKrRwJF0guRp"
   },
   "source": [
    "## Spark 和 Hadoop 对比\n",
    "\n",
    "- 时间\n",
    "    - Hadoop\n",
    "        1. 2006 年 1 月，Doug Cutting 加入 Yahoo，领导 Hadoop 的开发\n",
    "        2. 2008年1月，Hadoop成为Apache顶级项目\n",
    "        3. 2011年1.0正式发布\n",
    "        4. 2012年3月稳定版发布\n",
    "        5. 2013 年 10 月发布 2.X (Y arn)版本\n",
    "    - Spark\n",
    "        1. 2009年，Spark诞生于伯克利大学的AMPLab实验室\n",
    "        2. 2010年，伯克利大学正式开源了Spark项目\n",
    "        3. 2013年6月，Spark成为了Apache基金会下的项目\n",
    "        4. 2014年2月，Spark以飞快的速度成为了Apache的顶级项目\n",
    "        5. 2015年至今，Spark变得愈发火爆，大量的国内公司开始重点部署或者使用Spark\n",
    "\n",
    "    \n",
    "\n",
    "- 功能\n",
    "    - Hadoop\n",
    "        1. Hadoop是由java语言编写的，在分布式服务器集群上存储海量数据并运行分布式分析应用的开源框架\n",
    "        2. 作为 Hadoop 分布式文件系统，HDFS 处于 Hadoop 生态圈的最下层，存储着所有的数据，支持着 Hadoop 的所有服务。它的理论基础源于 Google 的TheGoogleFileSystem 这篇论文，它是 GFS 的开源实现。\n",
    "        3. MapReduce是一种编程模型，Hadoop根据Google的MapReduce论文将其实现， 作为 Hadoop 的分布式计算模型，是 Hadoop 的核心。基于这个框架，分布式并行 程序的编写变得异常简单。综合了 HDFS 的分布式存储和 MapReduce 的分布式计 算，Hadoop 在处理海量数据时，性能横向扩展变得非常容易。\n",
    "        4. HBase是对Google的Bigtable的开源实现，但又和Bigtable存在许多不同之处。 HBase 是一个基于 HDFS 的分布式数据库，擅长实时地随机读/写超大规模数据集。 它也是 Hadoop 非常重要的组件。\n",
    "    - Spark\n",
    "        1. Spark是一种由Scala语言开发的快速、通用、可扩展的大数据分析引擎\n",
    "        2. SparkCore中提供了Spark最基础与最核心的功能\n",
    "        3. Spark SQL是Spark用来操作结构化数据的组件。通过Spark SQL，用户可以使用 SQL 或者 Apache Hive 版本的 SQL 方言(HQL)来查询数据。\n",
    "        4. Spark Streaming 是 Spark 平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的 API。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 Spark 还是 Hadoop ？\n",
    "\n",
    "Hadoop 的 MR 框架和 Spark 框架都是数据处理框架，那么我们在使用时如何选择呢?\n",
    "\n",
    "- Hadoop MapReduce 由于其设计初衷并不是为了满足循环迭代式数据流处理，因此在多 并行运行的数据可复用场景(如:机器学习、图挖掘算法、交互式数据挖掘算法)中存 在诸多计算效率等问题。所以 Spark 应运而生，Spark 就是在传统的 MapReduce 计算框 架的基础上，利用其计算过程的优化，从而大大加快了数据分析、挖掘的运行和读写速度，并将计算单元缩小到更适合并行计算和重复使用的 RDD 计算模型。\n",
    "- 机器学习中 ALS、凸优化梯度下降等。这些都需要基于数据集或者数据集的衍生数据 反复查询反复操作。MR 这种模式不太合适，即使多 MR 串行处理，性能和时间也是一个问题。数据的共享依赖于磁盘。另外一种是交互式数据挖掘，MR 显然不擅长。而 Spark 所基于的 scala 语言恰恰擅长函数的处理。\n",
    "- Spark 是一个分布式数据快速分析项目。它的核心技术是弹性分布式数据集(Resilient Distributed Datasets)，提供了比 MapReduce 丰富的模型，可以快速在内存中对数据集 进行多次迭代，来支持复杂的数据挖掘算法和图形计算算法。\n",
    "- Spark 和 Hadoop 的根本差异是多个作业之间的数据通信问题 : Spark 多个作业之间数据通信是基于内存，而 Hadoop 是基于磁盘。\n",
    "- Spark Task 的启动时间快。Spark 采用 fork 线程的方式，而 Hadoop 采用创建新的进程的方式。\n",
    "- Spark 只有在 shuffle 的时候将数据写入磁盘，而 Hadoop 中多个 MR 作业之间的数据交互都要依赖于磁盘交互\n",
    "- Spark 的缓存机制比 HDFS 的缓存机制高效。\n",
    "\n",
    "经过上面的比较，我们可以看出在绝大多数的数据计算场景中，Spark 确实会比 MapReduce 更有优势。但是 Spark 是基于内存的，所以在实际的生产环境中，由于内存的限制，可能会 由于内存资源不够导致 Job 执行失败，此时，MapReduce 其实是一个更好的选择，所以 Spark 并不能完全替代 MR。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark 的核心模块\n",
    "\n",
    "![1613719473289.jpg](asset/1613719473289.jpg)\n",
    "\n",
    "- Spark Core\n",
    "\n",
    "    Spark Core 中提供了 Spark 最基础与最核心的功能，Spark 其他的功能如:Spark SQL，Spark Streaming，GraphX, MLlib 都是在 Spark Core 的基础上进行扩展的 \n",
    "- Spark SQL\n",
    "\n",
    "    Spark SQL 是 Spark 用来操作结构化数据的组件。通过 Spark SQL，用户可以使用 SQL 或者 Apache Hive 版本的 SQL 方言(HQL)来查询数据。\n",
    "- Spark Streaming\n",
    "\n",
    "    Spark Streaming 是 Spark 平台上针对实时数据进行流式计算的组件，提供了丰富的处理 数据流的 API。\n",
    "- Spark MLlib\n",
    "\n",
    "    MLlib 是 Spark 提供的一个机器学习算法库。MLlib 不仅提供了模型评估、数据导入等额外的功能，还提供了一些更底层的机器学习原语。\n",
    "- Spark GraphX\n",
    "\n",
    "    GraphX 是 Spark 面向图计算提供的框架与算法库。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark 快速上手"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 大数据世界中的 Hello World\n",
    "\n",
    "我们完成一个读取文件中的单词，并且统计单词出现个数的 Spark 程序，这个程序也是我们再大数据世界中的 Hello World。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了能够让 python 找到 pyspark，使用 findspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了使用 RDDs，创建 SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 SparkConf 和 SparkSession\n",
    "conf=SparkConf()\\\n",
    "        .setMaster('local[*]')\\\n",
    "        .setAppName(\"WordCount\")\\\n",
    "        .setExecutorEnv(\"spark.executor.memory\",\"2g\")\\\n",
    "        .setExecutorEnv(\"spark.driver.memory\",\"2g\")\n",
    "\n",
    "spark=SparkSession.builder\\\n",
    "        .config(conf=conf)\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取 SparkContext\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['First Citizen:',\n",
       " 'Before we proceed any further, hear me speak.',\n",
       " '',\n",
       " 'All:',\n",
       " 'Speak, speak.',\n",
       " '',\n",
       " 'First Citizen:',\n",
       " 'You are all resolved rather to die than to famish?',\n",
       " '',\n",
       " 'All:',\n",
       " 'Resolved. resolved.',\n",
       " '',\n",
       " 'First Citizen:',\n",
       " 'First, you know Caius Marcius is chief enemy to the people.',\n",
       " '',\n",
       " 'All:',\n",
       " \"We know't, we know't.\",\n",
       " '',\n",
       " 'First Citizen:',\n",
       " \"Let us kill him, and we'll have corn at our own price.\",\n",
       " \"Is't a verdict?\",\n",
       " '',\n",
       " 'All:',\n",
       " \"No more talking on't; let it be done: away, away!\",\n",
       " '',\n",
       " 'Second Citizen:',\n",
       " 'One word, good citizens.',\n",
       " '',\n",
       " 'First Citizen:',\n",
       " 'We are accounted poor citizens, the patricians good.',\n",
       " 'What authority surfeits on would relieve us: if they',\n",
       " 'would yield us but the superfluity, while it were',\n",
       " 'wholesome, we might guess they relieved us humanely;',\n",
       " 'but they think we are too dear: the leanness that',\n",
       " 'afflicts us, the object of our misery, is as an',\n",
       " 'inventory to particularise their abundance; our',\n",
       " 'sufferance is a gain to them Let us revenge this with',\n",
       " 'our pikes, ere we become rakes: for the gods know I',\n",
       " 'speak this in hunger for bread, not in thirst for revenge.',\n",
       " '',\n",
       " 'Second Citizen:',\n",
       " 'Would you proceed especially against Caius Marcius?',\n",
       " '',\n",
       " 'All:',\n",
       " \"Against him first: he's a very dog to the commonalty.\",\n",
       " '',\n",
       " 'Second Citizen:',\n",
       " 'Consider you what services he has done for his country?',\n",
       " '',\n",
       " 'First Citizen:']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取莎士比亚文本数据集, 先把 data/shakespeare.txt 文件上传到 hdfs 上\n",
    "shakespeare_path = \"/dataset/shakespeare.txt\"\n",
    "shakespeare_rdd=sc.textFile(shakespeare_path)\n",
    "shakespeare_rdd.take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取数据集的行数\n",
    "shakespeare_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 移除所有的标点符号\n",
    "# 把所有的单词转换成小写\n",
    "\n",
    "def lower_clean_str(x):\n",
    "    punc='!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~-'\n",
    "    lowercased_str = x.lower()\n",
    "    for ch in punc:\n",
    "        lowercased_str = lowercased_str.replace(ch, '')\n",
    "    return lowercased_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first citizen',\n",
       " 'before we proceed any further hear me speak',\n",
       " '',\n",
       " 'all',\n",
       " 'speak speak',\n",
       " '',\n",
       " 'first citizen',\n",
       " 'you are all resolved rather to die than to famish',\n",
       " '',\n",
       " 'all',\n",
       " 'resolved resolved',\n",
       " '',\n",
       " 'first citizen',\n",
       " 'first you know caius marcius is chief enemy to the people',\n",
       " '',\n",
       " 'all',\n",
       " 'we knowt we knowt',\n",
       " '',\n",
       " 'first citizen',\n",
       " 'let us kill him and well have corn at our own price',\n",
       " 'ist a verdict',\n",
       " '',\n",
       " 'all',\n",
       " 'no more talking ont let it be done away away',\n",
       " '',\n",
       " 'second citizen',\n",
       " 'one word good citizens',\n",
       " '',\n",
       " 'first citizen',\n",
       " 'we are accounted poor citizens the patricians good',\n",
       " 'what authority surfeits on would relieve us if they',\n",
       " 'would yield us but the superfluity while it were',\n",
       " 'wholesome we might guess they relieved us humanely',\n",
       " 'but they think we are too dear the leanness that',\n",
       " 'afflicts us the object of our misery is as an',\n",
       " 'inventory to particularise their abundance our',\n",
       " 'sufferance is a gain to them let us revenge this with',\n",
       " 'our pikes ere we become rakes for the gods know i',\n",
       " 'speak this in hunger for bread not in thirst for revenge',\n",
       " '',\n",
       " 'second citizen',\n",
       " 'would you proceed especially against caius marcius',\n",
       " '',\n",
       " 'all',\n",
       " 'against him first hes a very dog to the commonalty',\n",
       " '',\n",
       " 'second citizen',\n",
       " 'consider you what services he has done for his country',\n",
       " '',\n",
       " 'first citizen']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 给文本中的所有行，都执行 lower_clean_str 方法\n",
    "shakespeare_rdd = shakespeare_rdd.map(lower_clean_str)\n",
    "# 读取转换后的数据\n",
    "shakespeare_rdd.take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first',\n",
       " 'citizen',\n",
       " 'before',\n",
       " 'we',\n",
       " 'proceed',\n",
       " 'any',\n",
       " 'further',\n",
       " 'hear',\n",
       " 'me',\n",
       " 'speak',\n",
       " '',\n",
       " 'all',\n",
       " 'speak',\n",
       " 'speak',\n",
       " '']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 split 方法，把每一行中的单词分开（分词），并且把原来一行一行的数据“拉平”\n",
    "shakespeare_rdd = shakespeare_rdd.flatMap(lambda satir: satir.split(\" \"))\n",
    "shakespeare_rdd.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first',\n",
       " 'citizen',\n",
       " 'before',\n",
       " 'we',\n",
       " 'proceed',\n",
       " 'any',\n",
       " 'further',\n",
       " 'hear',\n",
       " 'me',\n",
       " 'speak',\n",
       " 'all',\n",
       " 'speak',\n",
       " 'speak',\n",
       " 'first',\n",
       " 'citizen']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 filter 方法，把分词后出现的空格符给过滤掉\n",
    "shakespeare_rdd = shakespeare_rdd.filter(lambda x:x!='')\n",
    "shakespeare_rdd.take(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算每一个单词出现的次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('first', 1),\n",
       " ('citizen', 1),\n",
       " ('before', 1),\n",
       " ('we', 1),\n",
       " ('proceed', 1),\n",
       " ('any', 1),\n",
       " ('further', 1),\n",
       " ('hear', 1),\n",
       " ('me', 1),\n",
       " ('speak', 1),\n",
       " ('all', 1),\n",
       " ('speak', 1),\n",
       " ('speak', 1),\n",
       " ('first', 1),\n",
       " ('citizen', 1)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 为了能够统计单词出现的次数，我们需要先把原来的 rdd 转换成 (word, 1) 这样的一对对的 rdd\n",
    "shakespeare_count = shakespeare_rdd.map(lambda  word:(word,1))\n",
    "shakespeare_count.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3', 27),\n",
       " ('a', 2987),\n",
       " ('abandond', 2),\n",
       " ('abase', 1),\n",
       " ('abate', 3),\n",
       " ('abated', 1),\n",
       " ('abbey', 1),\n",
       " ('abbot', 4),\n",
       " ('abed', 2),\n",
       " ('abels', 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 reduceByKey 来统计出每个单词出现的次数\n",
    "shakespeare_count_rbk = shakespeare_count.reduceByKey(lambda x,y:(x+y)).sortByKey()\n",
    "shakespeare_count_rbk.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(27, '3'),\n",
       " (2987, 'a'),\n",
       " (2, 'abandond'),\n",
       " (1, 'abase'),\n",
       " (3, 'abate'),\n",
       " (1, 'abated'),\n",
       " (1, 'abbey'),\n",
       " (4, 'abbot'),\n",
       " (2, 'abed'),\n",
       " (1, 'abels'),\n",
       " (1, 'abet'),\n",
       " (3, 'abhor'),\n",
       " (5, 'abhorrd'),\n",
       " (2, 'abhorred'),\n",
       " (1, 'abhorring')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 为了能够得到使用频次倒序排列的结果，我们要先把 shakespeare_count 转换成 (count, word)\n",
    "shakespeare_count_rbk = shakespeare_count_rbk.map(lambda x:(x[1],x[0]))\n",
    "shakespeare_count_rbk.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6283, 'the'),\n",
       " (5680, 'and'),\n",
       " (4766, 'to'),\n",
       " (4653, 'i'),\n",
       " (3757, 'of'),\n",
       " (3142, 'you'),\n",
       " (3118, 'my'),\n",
       " (2987, 'a'),\n",
       " (2569, 'that'),\n",
       " (2362, 'in')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 sortByKey 来获取 key 的倒序结果\n",
    "shakespeare_count_rbk.sortByKey(False).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看到这里，你应该能够得到想要的对于单词的统计结果了。但是，你可能还有很多的疑惑~没关系，接下来，我们就来一层层的剥开这些程序神秘的面纱，看看它到底是这样工作的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark 运行环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# 创建 SparkConf 和 SparkSession\n",
    "conf=SparkConf()\\\n",
    "        .setMaster('local[*]')\\\n",
    "        .setAppName(\"WordCount\")\\\n",
    "        .setExecutorEnv(\"spark.executor.memory\",\"2g\")\\\n",
    "        .setExecutorEnv(\"spark.driver.memory\",\"2g\")\n",
    "\n",
    "spark=SparkSession.builder\\\n",
    "        .config(conf=conf)\\\n",
    "        .getOrCreate()\n",
    "```\n",
    "\n",
    "在之前的代码中，我们有上面这样一段代码。这里我们执行了一个 setMaster 的函数，这个函数其实就是在设置 Spark 的运行环境的。接下来，让我们来了解下，Spark 的运行环境。\n",
    "\n",
    "<div align=\"center\"><img src=\"asset/1613724620336.jpg\" /></div>\n",
    "\n",
    "Spark 作为一个数据处理框架和计算引擎，被设计在所有常见的集群环境中运行, 在国内工作中主流的环境为 Yarn，不过逐渐容器式环境也慢慢流行起来。接下来，我们就分别看看不同环境下 Spark 的运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local 模式\n",
    "\n",
    "想啥呢，你之前一直在使用的模式可不是 Local 模式哟。所谓的 Local 模式，就是不需 要其他任何节点资源就可以在本地执行 Spark 代码的环境，一般用于教学，调试，演示等，我们之前使用的并不是这种。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standalone 模式\n",
    "\n",
    "local 本地模式毕竟只是用来进行练习演示的，真实工作中还是要将应用提交到对应的集群中去执行，这里我们来看看只使用 Spark 自身节点运行的集群模式，也就是我们所谓的 独立部署(Standalone)模式。Spark 的 Standalone 模式体现了经典的 master-slave 模式。 集群规划:\n",
    "\n",
    "<div align=\"center\"><img src=\"asset/1613725147329.jpg\" /></div>\n",
    "\n",
    "这也是我们部署的方式，这种方式，当我们在部署的机器上运行 jps 时，会有以下的一些进程。\n",
    "\n",
    "```\n",
    "================linux1================\n",
    "3330 Jps\n",
    "3238 Worker\n",
    "3163 Master\n",
    "================linux2================\n",
    "2966 Jps\n",
    "2908 Worker\n",
    "================linux3================\n",
    "2978 Worker\n",
    "3036 Jps\n",
    "```\n",
    "\n",
    "使用这种方式的时候，我们可以使用 Spark 自带的 UI 界面来管理 Job， UI 界面的地址为 http://<spark_host>:8080/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置高可用(HA)\n",
    "\n",
    "所谓的高可用是因为当前集群中的 Master 节点只有一个，所以会存在单点故障问题。所以为了解决单点故障问题，需要在集群中配置多个 Master 节点，一旦处于活动状态的 Master 发生故障时，由备用 Master 提供服务，保证作业可以继续执行。这里的高可用一般采用 Zookeeper 设置, 集群规划：\n",
    "\n",
    "<div align=\"center\"><img src=\"asset/1613725521838.jpg\" /></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yarn 模式\n",
    "\n",
    "独立部署(Standalone)模式由 Spark 自身提供计算资源，无需其他框架提供资源。这 种方式降低了和其他第三方资源框架的耦合性，独立性非常强。但是你也要记住，Spark 主 要是计算框架，而不是资源调度框架，所以本身提供的资源调度并不是它的强项，所以还是 和其他专业的资源调度框架集成会更靠谱一些。所以接下来我们来学习在强大的 Yarn 环境 下 Spark 是如何工作的(其实是因为在国内工作中，Yarn 使用的非常多)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K8S & Mesos 模式\n",
    "\n",
    "Mesos 是 Apache 下的开源分布式资源管理框架，它被称为是分布式系统的内核,在 Twitter 得到广泛使用,管理着 Twitter 超过 30,0000 台服务器上的应用部署，但是在国内，依然使用着传统的 Hadoop 大数据框架，所以国内使用 Mesos 框架的并不多，但是原理其实都差不多，这里我们就不做过多讲解了。\n",
    "\n",
    "<div align=\"center\"><img src=\"asset/1613725831934.jpg\" /></div>\n",
    "\n",
    "容器化部署是目前业界很流行的一项技术，基于 Docker 镜像运行能够让用户更加方便 地对应用进行管理和运维。容器管理工具中最为流行的就是 Kubernetes(k8s)，而 Spark 也在最近的版本中支持了 k8s 部署模式。这里我们也不做过多的讲解。给个链接大家自己感受一下:https://spark.apache.org/docs/latest/running-on-kubernetes.html\n",
    "\n",
    "<div align=\"center\"><img src=\"asset/1613725936826.jpg\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 部署模式对比\n",
    "\n",
    "<div align=\"center\"><img src=\"asset/1613726084985.jpg\" /></div>\n",
    "\n",
    "**使用到的一些端口号**\n",
    "\n",
    "- Spark查看当前Spark-shell运行任务情况端口号:4040(计算)\n",
    "- Spark Master 内部通信服务端口号:7077\n",
    "- Standalone模式下，SparkMasterWeb端口号:8080(资源)\n",
    "- Spark历史服务器端口号:18080\n",
    "- HadoopYARN任务运行情况查看端口号:8088"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark 运行架构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运行架构\n",
    "\n",
    "Spark 框架的核心是一个计算引擎，整体来说，它采用了标准 master-slave 的结构。\n",
    "\n",
    "如下图所示，它展示了一个 Spark 执行时的基本结构。图形中的 Driver 表示 master， 负责管理整个集群中的作业任务调度。图形中的 Executor 则是 slave，负责实际执行任务。\n",
    "\n",
    "<div align=\"center\"><img src=\"asset/1613726312570.jpg\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 核心组件\n",
    "\n",
    "由上图可以看出，对于 Spark 框架有两个核心组件:\n",
    "\n",
    "### Driver\n",
    "\n",
    "Spark 驱动器节点，用于执行 Spark 任务中的 main 方法，负责实际代码的执行工作。 Driver 在 Spark 作业执行时主要负责:\n",
    "\n",
    "- 将用户程序转化为作业(job)\n",
    "- 在Executor之间调度任务(task)\n",
    "- 跟踪Executor的执行情况\n",
    "- 通过UI展示查询运行情况\n",
    "\n",
    "实际上，我们无法准确地描述 Driver 的定义，因为在整个的编程过程中没有看到任何有关 Driver 的字眼。所以简单理解，所谓的 Driver 就是驱使整个应用运行起来的程序，也称之为 Driver 类。\n",
    "\n",
    "### Executor\n",
    "\n",
    "Spark Executor 是集群中工作节点(Worker)中的一个 JVM 进程，负责在 Spark 作业中运行具体任务(Task)，任务彼此之间相互独立。Spark 应用启动时，Executor 节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。如果有 Executor 节点发生了故障或崩溃，Spark 应用也可以继续执行，会将出错节点上的任务调度到其他 Executor 节点 上继续运行。\n",
    "\n",
    "Executor 有两个核心功能:\n",
    "\n",
    "- 负责运行组成Spark应用的任务，并将结果返回给驱动器进程\n",
    "- 它们通过自身的块管理器(Block Manager)为用户程序中要求缓存的 RDD 提供内存式存储。RDD 是直接缓存在 Executor 进程内的，因此任务可以在运行时充分利用缓存数据加速运算。\n",
    "\n",
    "### Master 和 Worker\n",
    "\n",
    "Spark 集群的独立部署环境中，不需要依赖其他的资源调度框架，自身就实现了资源调度的功能，所以环境中还有其他两个核心组件: aster 和 Worker，这里的 Master 是一个进程，主要负责资源的调度和分配，并进行集群的监控等职责，类似于 Yarn 环境中的 ResourceManager, 而 Worker 呢，也是进程，一个 Worker 运行在集群中的一台服务器上，由 Master 分配资源对数据进行并行的处理和计算，类似于 Yarn 环境中 NodeManager。\n",
    "\n",
    "\n",
    "### ApplicationMaster\n",
    "Hadoop 用户向 YARN 集群提交应用程序时,提交程序中应该包含 ApplicationMaster，用于向资源调度器申请执行任务的资源容器 Container，运行用户自己的程序任务 job，监控整 个任务的执行，跟踪整个任务的状态，处理任务失败等异常情况。\n",
    "说的简单点就是，ResourceManager(资源)和 Driver(计算)之间的解耦合靠的就是 ApplicationMaster。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 核心概念\n",
    "\n",
    "### Executor 与 Core\n",
    "\n",
    "Spark Executor 是集群中运行在工作节点(Worker)中的一个 JVM 进程，是整个集群中\n",
    "的专门用于计算的节点。在提交应用中，可以提供参数指定计算节点的个数，以及对应的资源。这里的资源一般指的是工作节点 Executor 的内存大小和使用的虚拟 CPU 核(Core)数量。\n",
    "\n",
    "应用程序相关启动参数如下:\n",
    "\n",
    "<div align=\"center\"><img src=\"asset/1613726781504.jpg\"/></div>\n",
    "\n",
    "### 并行度(Parallelism)\n",
    "\n",
    "在分布式计算框架中一般都是多个任务同时执行，由于任务分布在不同的计算节点进行 计算，所以能够真正地实现多任务并行执行，记住，这里是并行，而不是并发。这里我们将 整个集群并行执行任务的数量称之为并行度。那么一个作业到底并行度是多少呢?这个取决 于框架的默认配置。应用程序也可以在运行过程中动态修改。\n",
    "\n",
    "\n",
    "### 有向无环图(DAG)\n",
    "\n",
    "<div align=\"center\"><img src=\"asset/1613727224795.jpg\"/></div>\n",
    "\n",
    "大数据计算引擎框架我们根据使用方式的不同一般会分为四类，其中第一类就是 Hadoop 所承载的 MapReduce,它将计算分为两个阶段，分别为 Map 阶段和 Reduce 阶段。 对于上层应用来说，就不得不想方设法去拆分算法，甚至于不得不在上层应用实现多个 Job 的串联，以完成一个完整的算法，例如迭代计算。 由于这样的弊端，催生了支持 DAG 框架的产生。因此，支持 DAG 的框架被划分为第二代计算引擎。如 Tez 以及更上层的 Oozie。这里我们不去细究各种 DAG 实现之间的区别，不过对于当时的 Tez 和 Oozie 来说，大多还是批处理的任务。接下来就是以 Spark 为代表的第三代的计算引擎。第三代计算引擎的特点主要是 Job 内部的 DAG 支持(不跨越 Job)，以及实时计算。\n",
    "\n",
    "这里所谓的有向无环图，并不是真正意义的图形，而是由 Spark 程序直接映射成的数据 流的高级抽象模型。简单理解就是将整个程序计算的执行过程用图形表示出来,这样更直观， 更便于理解，可以用于表示程序的拓扑结构。\n",
    "\n",
    "DAG(Directed Acyclic Graph)有向无环图是由点和线组成的拓扑图形，该图形具有方向，不会闭环。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提交流程\n",
    "\n",
    "所谓的提交流程，其实就是我们开发人员根据需求写的应用程序通过 Spark 客户端提交给 Spark 运行环境执行计算的流程。在不同的部署环境中，这个提交过程基本相同，但是又有细微的区别，我们这里不进行详细的比较，但是因为国内工作中，将 Spark 引用部署到 Yarn 环境中会更多一些，所以本课程中的提交流程是基于 Yarn 环境的。\n",
    "\n",
    "<div align=\"center\"><img src=\"asset/1613727374528.jpg\"/></div>\n",
    "\n",
    "Spark 应用程序提交到 Yarn 环境中执行的时候，一般会有两种部署执行的方式:Client 和 Cluster。两种模式主要区别在于: Driver 程序的运行节点位置。\n",
    "\n",
    "\n",
    "### Yarn Client 模式\n",
    "\n",
    "Client 模式将用于监控和调度的 Driver 模块在客户端执行，而不是在 Yarn 中，所以一般用于测试。\n",
    "\n",
    "- Driver在任务提交的本地机器上运行\n",
    "- Driver启动后会和ResourceManager通讯申请启动ApplicationMaster\n",
    "- ResourceManager分配container，在合适的NodeManager上启动ApplicationMaster，负责向 ResourceManager 申请 Executor 内存\n",
    "- ResourceManager接到ApplicationMaster的资源申请后会分配container，然后 ApplicationMaster 在资源分配指定的 NodeManager 上启动 Executor 进程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark 核心编程\n",
    "\n",
    "Spark 计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。三大数据结构分别是:\n",
    "\n",
    "- RDD : 弹性分布式数据集\n",
    "- 累加器:分布式共享只写变量\n",
    "- 广播变量:分布式共享只读变量 接下来我们一起看看这三大数据结构是如何在数据处理中使用的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD\n",
    "\n",
    "### 什么是 RDD\n",
    "\n",
    "RDD(Resilient Distributed Dataset)叫做弹性分布式数据集，是 Spark 中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合。\n",
    "\n",
    "- 弹性\n",
    "    - 存储的弹性:内存与磁盘的自动切换; \n",
    "    - 容错的弹性:数据丢失可以自动恢复; \n",
    "    - 计算的弹性:计算出错重试机制;\n",
    "    - 分片的弹性:可根据需要重新分片。\n",
    "    \n",
    "    \n",
    "- 分布式:数据存储在大数据集群不同节点上\n",
    "- 数据集:RDD封装了计算逻辑，并不保存数据\n",
    "- 数据抽象:RDD是一个抽象类，需要子类具体实现\n",
    "- 不可变:RDD封装了计算逻辑，是不可以改变的，想要改变，只能产生新的RDD，在新的 RDD 里面封装计算逻辑\n",
    "- 可分区、并行计算\n",
    "\n",
    "### RDD 的核心属性\n",
    "\n",
    "- 分区列表\n",
    "\n",
    "    RDD 数据结构中存在分区列表，用于执行任务时并行计算，是实现分布式计算的重要属性。\n",
    "    \n",
    "- 分区计算函数\n",
    "\n",
    "    Spark 在计算时，是使用分区函数对每一个分区进行计算\n",
    "    \n",
    "- RDD之间的依赖关系\n",
    "    \n",
    "    RDD 是计算模型的封装，当需求中需要将多个计算模型进行组合时，就需要将多个 RDD 建 立依赖关系\n",
    "    \n",
    "- 分区器(可选)\n",
    "    \n",
    "    当数据为 KV 类型数据时，可以通过设定分区器自定义数据的分区\n",
    "    \n",
    "- 首选位置(可选)\n",
    "\n",
    "    计算数据时，可以根据计算节点的状态选择不同的节点位置进行计算\n",
    "    \n",
    "### RDD 执行原理\n",
    "\n",
    "从计算的角度来讲，数据处理过程中需要计算资源(内存 & CPU)和计算模型(逻辑)。执行时，需要将计算资源和计算模型进行协调和整合。Spark 框架在执行时，先申请资源，然后将应用程序的数据处理逻辑分解成一个一个的计算任务。然后将任务发到已经分配资源的计算节点上, 按照指定的计算模型进行数据计算。最后得到计算结果。RDD 是 Spark 框架中用于数据处理的核心模型，接下来我们看看，在 Yarn 环境中，RDD 的工作原理:\n",
    "\n",
    "1. 启动 Yarn 集群环境\n",
    "\n",
    "2. Spark 通过申请资源创建调度节点和计算节点\n",
    "\n",
    "3. Spark 框架根据需求将计算逻辑根据分区划分成不同的任务\n",
    "\n",
    "4. 调度节点将任务根据计算节点状态发送到对应的计算节点进行计算\n",
    "\n",
    "从以上流程可以看出 RDD 在整个流程中主要用于将逻辑进行封装，并生成 Task 发送给 Executor 节点执行计算，接下来我们就一起看看 Spark 框架中 RDD 是具体是如何进行数据处理的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark 基础编程\n",
    "\n",
    "开始编程之前，我们先来准备好 SparkContex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了能够让 python 找到 pyspark，使用 findspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# 为了使用 RDDs，创建 SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "# 创建 SparkConf 和 SparkSession\n",
    "conf=SparkConf()\\\n",
    "        .setMaster('local[*]')\\\n",
    "        .setAppName(\"WordCount\")\\\n",
    "        .setExecutorEnv(\"spark.executor.memory\",\"2g\")\\\n",
    "        .setExecutorEnv(\"spark.driver.memory\",\"2g\")\n",
    "\n",
    "spark=SparkSession.builder\\\n",
    "        .config(conf=conf)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# 获取 SparkContext\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD 创建\n",
    "\n",
    "在 Spark 中创建 RDD 的创建方式可以分为四种:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ①\n",
    "# 从集合(内存)中创建 RDD\n",
    "rdd1 = sc.parallelize([1, 2, 3, 4])\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['First Citizen:',\n",
       " 'Before we proceed any further, hear me speak.',\n",
       " '',\n",
       " 'All:',\n",
       " 'Speak, speak.',\n",
       " '',\n",
       " 'First Citizen:',\n",
       " 'You are all resolved rather to die than to famish?',\n",
       " '',\n",
       " 'All:']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ②\n",
    "# 从外部存储(文件)创建 RDD\n",
    "# 由外部存储系统的数据集创建 RDD 包括:本地的文件系统，所有 Hadoop 支持的数据集， 比如 HDFS、HBase 等。\n",
    "rdd2 = sc.textFile('/dataset/shakespeare.txt')\n",
    "rdd2.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "③从其他 RDD 创建\n",
    "主要是通过一个 RDD 运算完后，再产生新的 RDD。详情请参考后续章节\n",
    "\n",
    "④直接创建 RDD\n",
    "一般由 Spark 框架自身使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD 并行度和分区\n",
    "\n",
    "默认情况下，Spark 可以将一个作业切分多个任务后，发送给 Executor 节点并行计算，而能够并行计算的任务数量我们称之为并行度。这个数量可以在构建 RDD 时指定。记住，这里的并行执行的任务数量，并不是指的切分任务的数量，不要混淆了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 5\n",
      "Action: First element: 0\n"
     ]
    }
   ],
   "source": [
    "rdd3 = sc.parallelize(range(0, 6, 2), 5)\n",
    "rdd3.collect()\n",
    "\n",
    "print(\"Number of Partitions: \"+str(rdd3.getNumPartitions()))\n",
    "print(\"Action: First element: \"+str(rdd3.first()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里使我们后续做优化的要去调优的一个部分，所以关于怎么去设置并行和分区，后面在说原理的时候再来看。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD 转换算子\n",
    "\n",
    "RDD 根据数据处理方式的不同将算子整体上分为 Value 类型、双 Value 类型和 Key-Value 类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 1), ('c', 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. value 类型 \n",
    "# 将处理的数据逐条进行映射转换，这里的转换可以是类型的转换，也可以是值的转换。\n",
    "rdd4 = sc.parallelize([\"b\", \"a\", \"c\"])\n",
    "sorted(rdd4.map(lambda x: (x, 1)).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 7]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. value 类型\n",
    "# 将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据。\n",
    "def f(iterator): \n",
    "    yield sum(iterator)\n",
    "    \n",
    "rdd5 = sc.parallelize([1, 2, 3, 4], 2)\n",
    "rdd5.mapPartitions(f).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**思考一个问题:map 和 mapPartitions 的区别?**\n",
    "\n",
    "- 数据处理角度\n",
    "    Map 算子是分区内一个数据一个数据的执行，类似于串行操作。而 mapPartitions 算子是以分区为单位进行批处理操作。\n",
    "    \n",
    "- 功能的角度\n",
    "    Map 算子主要目的将数据源中的数据进行转换和改变。但是不会减少或增多数据。 MapPartitions 算子需要传递一个迭代器，返回一个迭代器，没有要求的元素的个数保持不变， 所以可以增加或减少数据\n",
    "    \n",
    "- 性能的角度\n",
    "    Map 算子因为类似于串行操作，所以性能比较低，而是 mapPartitions 算子类似于批处理，所以性能较高。但是 mapPartitions 算子会长时间占用内存，那么这样会导致内存可能不够用，出现内存溢出的错误。所以在内存有限的情况下，不推荐使用。使用 map 操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. value 类型\n",
    "# 将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据，在处理时同时可以获取当前分区索引。\n",
    "def f(splitIndex, iterator): \n",
    "    yield splitIndex\n",
    "\n",
    "rdd6 = sc.parallelize([1, 2, 3, 4], 4)\n",
    "rdd6.mapPartitionsWithIndex(f).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4]\n",
      "[1, 1, 1, 2, 2, 3]\n",
      "[(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n"
     ]
    }
   ],
   "source": [
    "# 4. value 类型\n",
    "# 将处理的数据进行扁平化后再进行映射处理，所以算子也称之为扁平映射\n",
    "rdd7 = sc.parallelize([2, 3, 4])\n",
    "print(rdd7.collect())\n",
    "print(sorted(rdd7.flatMap(lambda x: range(1, x)).collect()))\n",
    "print(sorted(rdd7.flatMap(lambda x: [(x, x), (x, x)]).collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. value 类型\n",
    "# 将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变\n",
    "rdd8 = sc.parallelize([1, 2, 3, 4], 2)\n",
    "sorted(rdd8.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [2, 8]), (1, [1, 1, 3, 5])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. value 类型\n",
    "# 将数据根据指定的规则进行分组, 分区默认不变，但是数据会被打乱重新组合，我们将这样的操作称之为 shuffle。极限情况下，数据可能被分在同一个分区中\n",
    "rdd9 = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
    "result = rdd9.groupBy(lambda x: x % 2).collect()\n",
    "sorted([(x, sorted(y)) for (x, y) in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. value 类型\n",
    "# 将数据根据指定的规则进行筛选过滤，符合规则的数据保留，不符合规则的数据丢弃。\n",
    "# 当数据进行筛选过滤后，分区不变，但是分区内的数据可能不均衡，生产环境下，可能会出现数据倾斜。\n",
    "rdd10 = sc.parallelize([1, 2, 3, 4, 5])\n",
    "rdd10.filter(lambda x: x % 2 == 0).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sample(withReplacement, fraction, seed=None)```\n",
    "\n",
    "\n",
    "Return a sampled subset of this RDD.\n",
    "\n",
    "Parameters\n",
    "- withReplacement – can elements be sampled multiple times (replaced when sampled out)\n",
    "\n",
    "- fraction – expected size of the sample as a fraction of this RDD’s size without replacement: probability that each element is chosen; fraction must be [0, 1] with replacement: expected number of times each element is chosen; fraction must be >= 0\n",
    "\n",
    "- seed – seed for the random number generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. value 类型\n",
    "# 根据指定的规则从数据集中抽取数据\n",
    "rdd11 = sc.parallelize(range(100), 4)\n",
    "6 <= rdd11.sample(False, 0.1, 81).count() <= 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9. value 类型 \n",
    "# 将数据集中重复的数据去重\n",
    "sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2, 3], [4, 5]]\n",
      "[[1, 2, 3, 4, 5]]\n"
     ]
    }
   ],
   "source": [
    "# 10. value 类型\n",
    "# 根据数据量缩减分区，用于大数据集过滤后，提高小数据集的执行效率\n",
    "# 当 spark 程序中，存在过多的小任务的时候，可以通过 coalesce 方法，收缩合并分区，减少分区的个数，减小任务调度成本\n",
    "print(sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect())\n",
    "print(sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2, 3], [4, 5], [6, 7]]\n",
      "2\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# 11. value 类型\n",
    "# 该操作内部其实执行的是 coalesce 操作，参数 shuffle 的默认值为 true。\n",
    "# 无论是将分区数多的 RDD 转换为分区数少的 RDD，还是将分区数少的 RDD 转换为分区数多的 RDD，\n",
    "# repartition 操作都可以完成，因为无论如何都会经 shuffle 过程。\n",
    "rdd12 = sc.parallelize([1,2,3,4,5,6,7], 4)\n",
    "print(sorted(rdd.glom().collect()))\n",
    "print(len(rdd12.repartition(2).glom().collect()))\n",
    "print(len(rdd12.repartition(10).glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      "[('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n"
     ]
    }
   ],
   "source": [
    "# 12. value 类型\n",
    "# 该操作用于排序数据。在排序之前，可以将数据通过 f 函数进行处理，\n",
    "# 之后按照 f 函数处理 的结果进行排序，默认为升序排列。\n",
    "# 排序后新产生的 RDD 的分区数与原 RDD 的分区数一致。中间存在 shuffle 的过程\n",
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "print(sc.parallelize(tmp).sortBy(lambda x: x[0]).collect())\n",
    "print(sc.parallelize(tmp).sortBy(lambda x: x[1]).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 3]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 双 value 类型\n",
    "# 对源 RDD 和参数 RDD 求交集后返回一个新的 RDD\n",
    "rdd13 = sc.parallelize([1, 10, 2, 3, 4, 5])\n",
    "rdd14 = sc.parallelize([1, 6, 2, 3, 7, 8])\n",
    "rdd13.intersection(rdd14).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 10, 2, 3, 4, 5, 1, 6, 2, 3, 7, 8]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 双 value 类型\n",
    "# 对源 RDD 和参数 RDD 求并集后返回一个新的 RDD\n",
    "rdd15 = sc.parallelize([1, 10, 2, 3, 4, 5])\n",
    "rdd16 = sc.parallelize([1, 6, 2, 3, 7, 8])\n",
    "rdd15.union(rdd16).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 4), ('b', 5)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 双 value 类型\n",
    "# 以一个 RDD 元素为主，去除两个 RDD 中重复元素，将其他元素保留下来。求差集\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
    "y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
    "sorted(x.subtract(y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. 双 value 类型\n",
    "# 将两个 RDD 中的元素，以键值对的形式进行合并。\n",
    "# 其中，键值对中的 Key 为第 1 个 RDD 中的元素，Value 为第 2 个 RDD 中的相同位置的元素。\n",
    "x = sc.parallelize(range(0,5))\n",
    "y = sc.parallelize(range(1000, 1005))\n",
    "x.zip(y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Key-Value 类型\n",
    "# 将数据按照指定 Partitioner 重新进行分区。Spark 默认的分区器是 HashPartitioner\n",
    "pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\n",
    "sets = pairs.partitionBy(2).glom().collect()\n",
    "len(set(sets[0]).intersection(set(sets[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 1)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Key-Value 类型\n",
    "# 可以将数据按照相同的 Key 对 Value 进行聚合\n",
    "from operator import add\n",
    "\n",
    "\n",
    "rdd17 = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd17.reduceByKey(add).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 2), ('b', 1)]\n",
      "[('a', [1, 1]), ('b', [1])]\n"
     ]
    }
   ],
   "source": [
    "# 3. Key-Value 类型\n",
    "# 将数据源的数据根据 key 对 value 进行分组\n",
    "rdd18 = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "print(sorted(rdd18.groupByKey().mapValues(len).collect()))\n",
    "print(sorted(rdd18.groupByKey().mapValues(list).collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**reduceByKey 和 groupByKey 的区别?**\n",
    "\n",
    "- 从 shuffle 的角度:\n",
    "    \n",
    "    reduceByKey 和 groupByKey 都存在 shuffle 的操作，但是 reduceByKey 可以在 shuffle 前对分区内相同 key 的数据进行预聚合(combine)功能，这样会减少落盘的 数据量，而 groupByKey 只是进行分组，不存在数据量减少的问题，reduceByKey 性能比较高。\n",
    "\n",
    "- 从功能的角度:\n",
    "    \n",
    "    reduceByKey 其实包含分组和聚合的功能。GroupByKey 只能分组，不能聚合，所以在分组聚合的场合下，推荐使用 reduceByKey，如果仅仅是分组而不需要聚合。那么还是只能使用 groupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Key-Value 类型\n",
    "# 将数据根据不同的规则进行分区内计算和分区间计算\n",
    "# aggregateByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 1)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Key-Value 类型\n",
    "# 当分区内计算规则和分区间计算规则相同时，aggregateByKey 就可以简化为 foldByKey\n",
    "from operator import add\n",
    "\n",
    "\n",
    "rdd19 = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd19.foldByKey(0, add).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```combineByKey(createCombiner, mergeValue, mergeCombiners, numPartitions=None, partitionFunc=<function portable_hash>)```\n",
    "\n",
    "Generic function to combine the elements for each key using a custom set of aggregation functions.\n",
    "\n",
    "Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a “combined type” C.\n",
    "\n",
    "Users provide three functions:\n",
    "\n",
    "- createCombiner, which turns a V into a C (e.g., creates a one-element list)\n",
    "\n",
    "- mergeValue, to merge a V into a C (e.g., adds it to the end of a list)\n",
    "\n",
    "- mergeCombiners, to combine two C’s into a single one (e.g., merges the lists)\n",
    "\n",
    "To avoid memory allocation, both mergeValue and mergeCombiners are allowed to modify and return their first argument instead of creating a new C.\n",
    "\n",
    "In addition, users can control the partitioning of the output RDD.\n",
    "\n",
    "> Note V and C can be different – for example, one might group an RDD of type (Int, Int) into an RDD of type (Int, List[Int])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [1, 2]), ('b', [1])]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Key-Value 类型\n",
    "# 最通用的对 key-value 型 rdd 进行聚集操作的聚集函数(aggregation function)。\n",
    "# 类似于 aggregate()，combineByKey()允许用户返回值的类型与输入不一致。\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
    "\n",
    "def to_list(a):\n",
    "    return [a]\n",
    "\n",
    "def append(a, b):\n",
    "    a.append(b)\n",
    "    return a\n",
    "\n",
    "def extend(a, b):\n",
    "    a.extend(b)\n",
    "    return a\n",
    "\n",
    "sorted(x.combineByKey(to_list, append, extend).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**reduceByKey、foldByKey、aggregateByKey、combineByKey 的区别?**\n",
    "\n",
    "- reduceByKey: 相同 key 的第一个数据不进行任何计算，分区内和分区间计算规则相同 \n",
    "- FoldByKey: 相同 key 的第一个数据和初始值进行分区内计算，分区内和分区间计算规则相同\n",
    "- AggregateByKey:相同 key 的第一个数据和初始值进行分区内计算，分区内和分区间计算规 则可以不相同 \n",
    "- CombineByKey:当计算时，发现数据结构不满足要求时，可以让第一个数据转换结构。分区内和分区间计算规则不相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', 3)\n",
      "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      "[('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5), ('little', 4), ('Mary', 1), ('was', 8), ('white', 9), ('whose', 6)]\n"
     ]
    }
   ],
   "source": [
    "# 7. Key-Value 类型\n",
    "# 在一个(K,V)的 RDD 上调用，K 必须实现 Ordered 接口(特质)，返回一个按照 key 进行排序的\n",
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "print(sc.parallelize(tmp).sortByKey().first())\n",
    "print(sc.parallelize(tmp).sortByKey(True, 1).collect())\n",
    "print(sc.parallelize(tmp).sortByKey(True, 2).collect())\n",
    "tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
    "tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
    "print(sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 2)), ('a', (1, 3))]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. Key-Value 类型\n",
    "# 在类型为(K,V)和(K,W)的 RDD 上调用，返回一个相同 key 对应的所有元素连接在一起的 (K,(V,W)) 的 RDD\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
    "sorted(x.join(y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 2)), ('b', (4, None))]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9. Key-Value 类型\n",
    "# 类似于 SQL 语句的左外连接\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2)])\n",
    "sorted(x.leftOuterJoin(y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', ([1], [2])), ('b', ([4], []))]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10. Key-Value 类型\n",
    "# 在类型为 (K,V) 和 (K,W) 的 RDD 上调用，返回一个(K,(Iterable<V>,Iterable<W>))类型的 RDD\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2)])\n",
    "[(x, tuple(map(list, y))) for x, y in sorted(list(x.cogroup(y).collect()))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD 动作算子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "# 聚集 RDD 中的所有元素，先聚合分区内数据，再聚合分区间数据\n",
    "from operator import add\n",
    "\n",
    "\n",
    "print(sc.parallelize([1, 2, 3, 4, 5]).reduce(add))\n",
    "print(sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 \n",
    "# 在驱动程序中，以数组 Array 的形式返回数据集的所有元素\n",
    "# collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3\n",
    "# 返回 RDD 中元素的个数\n",
    "sc.parallelize([2, 3, 4]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4\n",
    "# 返回 RDD 中的第一个元素\n",
    "sc.parallelize([2, 3, 4]).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3]\n",
      "[2, 3, 4, 5, 6]\n",
      "[91, 92, 93]\n"
     ]
    }
   ],
   "source": [
    "# 5\n",
    "# 返回一个由 RDD 的前 n 个元素组成的数组\n",
    "print(sc.parallelize([2, 3, 4, 5, 6]).cache().take(2))\n",
    "print(sc.parallelize([2, 3, 4, 5, 6]).take(10))\n",
    "print(sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6]\n",
      "[10, 9, 7, 6, 5, 4]\n"
     ]
    }
   ],
   "source": [
    "# 6 \n",
    "# 返回该 RDD 排序后的前 n 个元素组成的数组\n",
    "print(sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6))\n",
    "print(sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4)\n",
      "(0, 0)\n"
     ]
    }
   ],
   "source": [
    "# 7 \n",
    "# 分区的数据通过初始值和分区内的数据进行聚合，然后再和初始值进行分区间的数据聚合\n",
    "seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
    "combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "print(sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp))\n",
    "print(sc.parallelize([]).aggregate((0, 0), seqOp, combOp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8\n",
    "# 折叠操作，aggregate 的简化版操作\n",
    "from operator import add\n",
    "\n",
    "sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 1)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9\n",
    "# 统计每种 key 的个数\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd.countByKey().items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**save 系列**\n",
    "\n",
    "```saveAsHadoopDataset(conf, keyConverter=None, valueConverter=None)```\n",
    "Output a Python RDD of key-value pairs (of form RDD[(K, V)]) to any Hadoop file system, using the old Hadoop OutputFormat API (mapred package). Keys/values are converted for output using either user specified converters or, by default, “org.apache.spark.api.python.JavaToWritableConverter”.\n",
    "\n",
    "Parameters\n",
    "- conf – Hadoop job configuration, passed in as a dict\n",
    "\n",
    "- keyConverter – (None by default)\n",
    "\n",
    "- valueConverter – (None by default)\n",
    "\n",
    "\n",
    "```saveAsHadoopFile(path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None, compressionCodecClass=None)```\n",
    "Output a Python RDD of key-value pairs (of form RDD[(K, V)]) to any Hadoop file system, using the old Hadoop OutputFormat API (mapred package). Key and value types will be inferred if not specified. Keys and values are converted for output using either user specified converters or “org.apache.spark.api.python.JavaToWritableConverter”. The conf is applied on top of the base Hadoop conf associated with the SparkContext of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
    "\n",
    "Parameters\n",
    "- path – path to Hadoop file\n",
    "\n",
    "- outputFormatClass – fully qualified classname of Hadoop OutputFormat (e.g. “org.apache.hadoop.mapred.SequenceFileOutputFormat”)\n",
    "\n",
    "- keyClass – fully qualified classname of key Writable class (e.g. “org.apache.hadoop.io.IntWritable”, None by default)\n",
    "\n",
    "- valueClass – fully qualified classname of value Writable class (e.g. “org.apache.hadoop.io.Text”, None by default)\n",
    "\n",
    "- keyConverter – (None by default)\n",
    "\n",
    "- valueConverter – (None by default)\n",
    "\n",
    "- conf – (None by default)\n",
    "\n",
    "- compressionCodecClass – (None by default)\n",
    "\n",
    "\n",
    "```saveAsNewAPIHadoopDataset(conf, keyConverter=None, valueConverter=None)```\n",
    "Output a Python RDD of key-value pairs (of form RDD[(K, V)]) to any Hadoop file system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are converted for output using either user specified converters or, by default, “org.apache.spark.api.python.JavaToWritableConverter”.\n",
    "\n",
    "Parameters\n",
    "- conf – Hadoop job configuration, passed in as a dict\n",
    "\n",
    "- keyConverter – (None by default)\n",
    "\n",
    "- valueConverter – (None by default)\n",
    "\n",
    "```saveAsNewAPIHadoopFile(path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None)```\n",
    "Output a Python RDD of key-value pairs (of form RDD[(K, V)]) to any Hadoop file system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types will be inferred if not specified. Keys and values are converted for output using either user specified converters or “org.apache.spark.api.python.JavaToWritableConverter”. The conf is applied on top of the base Hadoop conf associated with the SparkContext of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
    "\n",
    "Parameters\n",
    "- path – path to Hadoop file\n",
    "\n",
    "- outputFormatClass – fully qualified classname of Hadoop OutputFormat (e.g. “org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat”)\n",
    "\n",
    "- keyClass – fully qualified classname of key Writable class (e.g. “org.apache.hadoop.io.IntWritable”, None by default)\n",
    "\n",
    "- valueClass – fully qualified classname of value Writable class (e.g. “org.apache.hadoop.io.Text”, None by default)\n",
    "\n",
    "- keyConverter – (None by default)\n",
    "\n",
    "- valueConverter – (None by default)\n",
    "\n",
    "- conf – Hadoop job configuration, passed in as a dict (None by default)\n",
    "\n",
    "```saveAsPickleFile(path, batchSize=10)```\n",
    "Save this RDD as a SequenceFile of serialized objects. The serializer used is pyspark.serializers.PickleSerializer, default batch size is 10.\n",
    "\n",
    "\n",
    "```saveAsSequenceFile(path, compressionCodecClass=None)```\n",
    "Output a Python RDD of key-value pairs (of form RDD[(K, V)]) to any Hadoop file system, using the “org.apache.hadoop.io.Writable” types that we convert from the RDD’s key and value types. The mechanism is as follows:\n",
    "\n",
    "Pyrolite is used to convert pickled Python RDD into RDD of Java objects.\n",
    "\n",
    "Keys and values of this Java RDD are converted to Writables and written out.\n",
    "\n",
    "Parameters\n",
    "- path – path to sequence file\n",
    "\n",
    "- compressionCodecClass – (None by default)\n",
    "\n",
    "```saveAsTextFile(path, compressionCodecClass=None)```\n",
    "Save this RDD as a text file, using string representations of elements.\n",
    "\n",
    "Parameters\n",
    "- path – path to text file\n",
    "\n",
    "- compressionCodecClass – (None by default) string i.e. “org.apache.hadoop.io.compress.GzipCodec”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11\n",
    "# 分布式遍历 RDD 中的每一个元素，调用指定函数\n",
    "def f(x):\n",
    "    print(x)\n",
    "    \n",
    "sc.parallelize([1, 2, 3, 4, 5]).foreach(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD 的依赖关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD 血缘关系\n",
    "\n",
    "RDD 只支持粗粒度转换，即在大量记录上执行的单个操作。将创建 RDD 的一系列 Lineage (血统)记录下来，以便恢复丢失的分区。RDD 的 Lineage 会记录 RDD 的元数据信息和转 换行为，当该 RDD 的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的 数据分区。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD 依赖关系\n",
    "\n",
    "这里所谓的依赖关系，其实就是两个相邻 RDD 之间的关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 窄依赖\n",
    "\n",
    "窄依赖表示每一个父(上游)RDD 的 Partition 最多被子(下游)RDD 的一个 Partition 使用，窄依赖我们形象的比喻为独生子女。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 宽依赖\n",
    "\n",
    "宽依赖表示同一个父(上游)RDD 的 Partition 被多个子(下游)RDD 的 Partition 依赖，会 引起 Shuffle，总结:宽依赖我们形象的比喻为多生。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD 阶段划分\n",
    "\n",
    "DAG(Directed Acyclic Graph)有向无环图是由点和线组成的拓扑图形，该图形具有方向， 不会闭环。例如，DAG 记录了 RDD 的转换过程和任务的阶段。\n",
    "\n",
    "<div align=\"center\"><img src=\"asset/1613809284051.jpg\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD 任务划分\n",
    "\n",
    "RDD 任务切分中间分为:Application、Job、Stage 和 Task\n",
    "\n",
    "- Application:初始化一个SparkContext即生成一个Application;\n",
    "- Job:一个Action算子就会生成一个Job;\n",
    "- Stage:Stage等于宽依赖(ShuffleDependency)的个数加1;\n",
    "- Task:一个Stage阶段中，最后一个RDD的分区个数就是Task的个数。\n",
    "\n",
    "注意:Application->Job->Stage->Task 每一层都是 1 对 n 的关系。\n",
    "\n",
    "<div align=\"center\"><img src=\"asset/1613809448691.jpg\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD 的持久化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Cache 缓存\n",
    "\n",
    "RDD 通过 Cache 或者 Persist 方法将前面的计算结果缓存，默认情况下会把数据以缓存在 JVM 的堆内存中。但是并不是这两个方法被调用时立即缓存，而是触发后面的 action 算子时，该 RDD 将会被缓存在计算节点的内存中，并供后面重用。\n",
    "\n",
    "<div align=\"center\"><img src=\"asset/1613809558769.jpg\"/></div>\n",
    "\n",
    "缓存有可能丢失，或者存储于内存的数据由于内存不足而被删除，RDD 的缓存容错机 制保证了即使缓存丢失也能保证计算的正确执行。通过基于 RDD 的一系列转换，丢失的数据会被重算，由于 RDD 的各个 Partition 是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部 Partition。\n",
    "Spark 会自动对一些 Shuffle 操作的中间数据做持久化操作(比如:reduceByKey)。这样做的目的是为了当一个节点 Shuffle 失败了避免重新计算整个输入。但是，在实际使用的时候，如果想重用数据，仍然建议调用 persist 或 cache。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD CheckPoint 检查点\n",
    "\n",
    "所谓的检查点其实就是通过将 RDD 中间结果写入磁盘，由于血缘依赖过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果检查点之后有节点出现问题，可以从检查点开始重做血缘，减少了开销。\n",
    "\n",
    "对 RDD 进行 checkpoint 操作并不会马上被执行，必须执行 Action 操作才能触发。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 缓存和检查点区别\n",
    "\n",
    "- Cache 缓存只是将数据保存起来，不切断血缘依赖。Checkpoint 检查点切断血缘依赖。 \n",
    "- Cache 缓存的数据通常存储在磁盘、内存等地方，可靠性低。Checkpoint 的数据通常存 储在 HDFS 等容错、高可用的文件系统，可靠性高。\n",
    "- 建议对 checkpoint() 的 RDD 使用 Cache 缓存，这样 checkpoint 的 job 只需从 Cache 缓存中读取数据即可，否则需要再从头计算一次 RDD。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD 分区器\n",
    "\n",
    "Spark 目前支持 Hash 分区和 Range 分区，和用户自定义分区。Hash 分区为当前的默认\n",
    "分区。分区器直接决定了 RDD 中分区的个数、RDD 中每条数据经过 Shuffle 后进入哪个分 区，进而决定了 Reduce 的个数。\n",
    "\n",
    "- 只有 Key-Value 类型的RDD才有分区器，非Key-Value类型的RDD分区的值是None \n",
    "- 每个 RDD 的分区 ID 范围:0 ~ (numPartitions - 1)，决定这个值是属于那个分区的。\n",
    "\n",
    "1) Hash 分区:对于给定的 key，计算其 hashCode,并除以分区个数取余\n",
    "2) Range 分区:将一定范围内的数据映射到一个分区中，尽量保证每个分区数据均匀，而且分区间有序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 累加器\n",
    "\n",
    "累加器用来把 Executor 端变量信息聚合到 Driver 端。在 Driver 程序中定义的变量，在 Executor 端的每个 Task 都会得到这个变量的一份新的副本，每个 task 更新这些副本的值后， 传回 Driver 端进行 merge。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 广播变量\n",
    "\n",
    "广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个 Spark 操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表， 广播变量用起来都很顺手。在多个并行操作中使用同一个变量，但是 Spark 会为每个任务分别发送。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark 企业实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hive 中创建用户播放视频表。\n",
    "\n",
    "```sql\n",
    "create table cilicili.user_play \n",
    "(\n",
    "    id INT, \n",
    "    user_id INT, \n",
    "    content_id INT, \n",
    "    play_time INT\n",
    ")\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t';\n",
    "```\n",
    "\n",
    "安装 datax，并增加 job 如下：\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"job\": {\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"reader\": {\n",
    "                    \"name\": \"mysqlreader\",\n",
    "                    \"parameter\": {\n",
    "                        \"column\": [\n",
    "                            \"id\",\n",
    "                            \"user_id\",\n",
    "                            \"content_id\",\n",
    "                            \"play_time\"\n",
    "                        ],\n",
    "                        \"connection\": [\n",
    "                            {\n",
    "                                \"jdbcUrl\": [\n",
    "                                    \"jdbc:mysql://192.168.56.101:3306/my_project\"\n",
    "                                ],\n",
    "                                \"table\": [\n",
    "                                    \"user_play\"\n",
    "                                ]\n",
    "                            }\n",
    "                        ],\n",
    "                        \"password\": \"bigdata123\",\n",
    "                        \"username\": \"bigdata\",\n",
    "                        \"where\": \"\"\n",
    "                    }\n",
    "                },\n",
    "                \"writer\": {\n",
    "                    \"name\": \"hdfswriter\",\n",
    "                    \"parameter\": {\n",
    "                        \"column\": [\n",
    "                            {\n",
    "                                \"name\": \"id\",\n",
    "                                \"type\": \"INT\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"name\": \"user_id\",\n",
    "                                \"type\": \"INT\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"name\": \"content_id\",\n",
    "                                \"type\": \"INT\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"name\": \"play_time\",\n",
    "                                \"type\": \"FLOAT\"\n",
    "                            }\n",
    "                        ],\n",
    "                        \"compress\": \"gzip\",\n",
    "                        \"defaultFS\": \"hdfs://bigdata-node1:9820\",\n",
    "                        \"fieldDelimiter\": \"\\t\",\n",
    "                        \"fileName\": \"user\",\n",
    "                        \"fileType\": \"text\",\n",
    "                        \"path\": \"/hive/warehouse/cilicili.db/user_play\",\n",
    "                        \"writeMode\": \"append\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"setting\": {\n",
    "            \"speed\": {\n",
    "                \"channel\": \"1\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "配置文件中 `\"path\": \"/hive/warehouse/cilicili.db/user_play\"` 地址，就是我们使用 hive 创建的表的文件存储地址。\n",
    "\n",
    "确保使用的 mysql 版本和 jdbc mysql 的驱动版本一致。\n",
    "\n",
    "```bash\n",
    "mv datax/plugin/reader/mysqlreader/libs/mysql-connector-java-5.1.34.jar plugin/reader/mysqlreader/libs/mysql-connector-java-8.x.x.jar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1\\t40\\t15094\\t44.0',\n",
       " '2\\t40\\t12044\\t29.0',\n",
       " '3\\t40\\t8419\\t52.0',\n",
       " '4\\t40\\t7769\\t25.0',\n",
       " '5\\t40\\t17174\\t49.0',\n",
       " '6\\t40\\t7735\\t60.0',\n",
       " '7\\t40\\t9266\\t35.0',\n",
       " '8\\t40\\t4862\\t16.0',\n",
       " '9\\t40\\t10480\\t57.0',\n",
       " '10\\t40\\t10919\\t21.0']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 为了能够让 python 找到 pyspark，使用 findspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# 为了使用 RDDs，创建 SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "# 创建 SparkConf 和 SparkSession\n",
    "conf=SparkConf()\\\n",
    "        .setMaster('local[*]')\\\n",
    "        .setAppName(\"Spark Read Hive\")\\\n",
    "        .setExecutorEnv(\"spark.executor.memory\",\"4g\")\\\n",
    "        .setExecutorEnv(\"spark.driver.memory\",\"4g\")\n",
    "\n",
    "spark=SparkSession.builder\\\n",
    "        .config(conf=conf)\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# 获取 SparkContext\n",
    "sc=spark.sparkContext\n",
    "\n",
    "user_play_rdd = sc.textFile('/hive/warehouse/cilicili.db/user_play')\n",
    "user_play_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '40', '15094', '44.0'],\n",
       " ['2', '40', '12044', '29.0'],\n",
       " ['3', '40', '8419', '52.0'],\n",
       " ['4', '40', '7769', '25.0'],\n",
       " ['5', '40', '17174', '49.0'],\n",
       " ['6', '40', '7735', '60.0'],\n",
       " ['7', '40', '9266', '35.0'],\n",
       " ['8', '40', '4862', '16.0'],\n",
       " ['9', '40', '10480', '57.0'],\n",
       " ['10', '40', '10919', '21.0']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 先把它切断\n",
    "user_play_rdd = user_play_rdd.map(lambda satir: satir.split(\"\\t\"))\n",
    "user_play_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ContentId='4012', sum=602),\n",
       " Row(ContentId='10702', sum=446),\n",
       " Row(ContentId='17308', sum=426),\n",
       " Row(ContentId='5773', sum=426),\n",
       " Row(ContentId='13307', sum=420),\n",
       " Row(ContentId='10680', sum=408),\n",
       " Row(ContentId='6896', sum=396),\n",
       " Row(ContentId='9453', sum=394),\n",
       " Row(ContentId='15493', sum=392),\n",
       " Row(ContentId='10649', sum=390),\n",
       " Row(ContentId='6923', sum=388),\n",
       " Row(ContentId='12618', sum=388),\n",
       " Row(ContentId='10061', sum=382),\n",
       " Row(ContentId='10040', sum=382),\n",
       " Row(ContentId='4276', sum=382),\n",
       " Row(ContentId='9434', sum=380),\n",
       " Row(ContentId='14907', sum=379),\n",
       " Row(ContentId='5062', sum=379),\n",
       " Row(ContentId='11026', sum=379),\n",
       " Row(ContentId='4638', sum=378),\n",
       " Row(ContentId='6662', sum=377),\n",
       " Row(ContentId='7616', sum=375),\n",
       " Row(ContentId='7882', sum=373),\n",
       " Row(ContentId='8405', sum=371),\n",
       " Row(ContentId='10685', sum=368),\n",
       " Row(ContentId='14186', sum=367),\n",
       " Row(ContentId='16870', sum=365),\n",
       " Row(ContentId='17322', sum=363),\n",
       " Row(ContentId='6126', sum=363),\n",
       " Row(ContentId='8317', sum=362),\n",
       " Row(ContentId='8094', sum=361),\n",
       " Row(ContentId='8845', sum=360),\n",
       " Row(ContentId='16604', sum=359),\n",
       " Row(ContentId='9694', sum=359),\n",
       " Row(ContentId='13664', sum=355),\n",
       " Row(ContentId='12412', sum=353),\n",
       " Row(ContentId='7596', sum=352),\n",
       " Row(ContentId='14737', sum=351),\n",
       " Row(ContentId='11618', sum=350),\n",
       " Row(ContentId='15501', sum=349),\n",
       " Row(ContentId='12401', sum=348),\n",
       " Row(ContentId='5760', sum=347),\n",
       " Row(ContentId='7548', sum=347),\n",
       " Row(ContentId='5269', sum=346),\n",
       " Row(ContentId='10734', sum=345),\n",
       " Row(ContentId='5638', sum=344),\n",
       " Row(ContentId='9351', sum=344),\n",
       " Row(ContentId='16882', sum=343),\n",
       " Row(ContentId='12030', sum=342),\n",
       " Row(ContentId='17542', sum=342),\n",
       " Row(ContentId='13734', sum=341),\n",
       " Row(ContentId='12095', sum=339),\n",
       " Row(ContentId='7198', sum=339),\n",
       " Row(ContentId='5448', sum=338),\n",
       " Row(ContentId='17093', sum=338),\n",
       " Row(ContentId='5777', sum=338),\n",
       " Row(ContentId='16231', sum=338),\n",
       " Row(ContentId='7503', sum=337),\n",
       " Row(ContentId='16588', sum=336),\n",
       " Row(ContentId='8463', sum=335),\n",
       " Row(ContentId='15885', sum=335),\n",
       " Row(ContentId='15607', sum=335),\n",
       " Row(ContentId='4173', sum=335),\n",
       " Row(ContentId='7580', sum=334),\n",
       " Row(ContentId='14230', sum=334),\n",
       " Row(ContentId='8595', sum=334),\n",
       " Row(ContentId='8589', sum=333),\n",
       " Row(ContentId='15790', sum=333),\n",
       " Row(ContentId='7963', sum=332),\n",
       " Row(ContentId='15959', sum=331),\n",
       " Row(ContentId='7060', sum=331),\n",
       " Row(ContentId='4913', sum=331),\n",
       " Row(ContentId='8112', sum=331),\n",
       " Row(ContentId='17138', sum=331),\n",
       " Row(ContentId='15666', sum=330),\n",
       " Row(ContentId='6247', sum=330),\n",
       " Row(ContentId='5512', sum=329),\n",
       " Row(ContentId='17839', sum=329),\n",
       " Row(ContentId='6006', sum=328),\n",
       " Row(ContentId='7920', sum=328),\n",
       " Row(ContentId='9747', sum=328),\n",
       " Row(ContentId='6663', sum=328),\n",
       " Row(ContentId='14419', sum=327),\n",
       " Row(ContentId='8599', sum=326),\n",
       " Row(ContentId='10610', sum=326),\n",
       " Row(ContentId='11229', sum=326),\n",
       " Row(ContentId='17020', sum=325),\n",
       " Row(ContentId='12190', sum=325),\n",
       " Row(ContentId='12369', sum=325),\n",
       " Row(ContentId='9815', sum=324),\n",
       " Row(ContentId='11404', sum=324),\n",
       " Row(ContentId='7769', sum=323),\n",
       " Row(ContentId='14702', sum=323),\n",
       " Row(ContentId='16792', sum=322),\n",
       " Row(ContentId='6519', sum=322),\n",
       " Row(ContentId='6149', sum=321),\n",
       " Row(ContentId='5359', sum=321),\n",
       " Row(ContentId='7468', sum=321),\n",
       " Row(ContentId='13127', sum=320),\n",
       " Row(ContentId='10195', sum=320),\n",
       " Row(ContentId='6127', sum=320),\n",
       " Row(ContentId='14313', sum=320),\n",
       " Row(ContentId='14864', sum=320),\n",
       " Row(ContentId='17216', sum=318),\n",
       " Row(ContentId='10728', sum=318),\n",
       " Row(ContentId='8775', sum=318),\n",
       " Row(ContentId='15073', sum=318),\n",
       " Row(ContentId='7320', sum=318),\n",
       " Row(ContentId='10586', sum=317),\n",
       " Row(ContentId='12248', sum=315),\n",
       " Row(ContentId='5541', sum=315),\n",
       " Row(ContentId='15362', sum=315),\n",
       " Row(ContentId='6996', sum=315),\n",
       " Row(ContentId='7967', sum=315),\n",
       " Row(ContentId='5614', sum=315),\n",
       " Row(ContentId='5977', sum=314),\n",
       " Row(ContentId='16062', sum=314),\n",
       " Row(ContentId='15091', sum=314),\n",
       " Row(ContentId='14266', sum=314),\n",
       " Row(ContentId='14133', sum=314),\n",
       " Row(ContentId='12340', sum=314),\n",
       " Row(ContentId='15748', sum=313),\n",
       " Row(ContentId='13811', sum=313),\n",
       " Row(ContentId='17673', sum=313),\n",
       " Row(ContentId='12569', sum=313),\n",
       " Row(ContentId='12939', sum=312),\n",
       " Row(ContentId='17387', sum=311),\n",
       " Row(ContentId='8797', sum=311),\n",
       " Row(ContentId='15933', sum=310),\n",
       " Row(ContentId='12459', sum=310),\n",
       " Row(ContentId='9114', sum=310),\n",
       " Row(ContentId='5727', sum=310),\n",
       " Row(ContentId='13268', sum=309),\n",
       " Row(ContentId='5647', sum=309),\n",
       " Row(ContentId='5572', sum=309),\n",
       " Row(ContentId='9464', sum=308),\n",
       " Row(ContentId='5335', sum=308),\n",
       " Row(ContentId='17593', sum=307),\n",
       " Row(ContentId='7094', sum=306),\n",
       " Row(ContentId='14764', sum=306),\n",
       " Row(ContentId='9243', sum=306),\n",
       " Row(ContentId='4550', sum=306),\n",
       " Row(ContentId='17429', sum=306),\n",
       " Row(ContentId='8120', sum=305),\n",
       " Row(ContentId='14677', sum=305),\n",
       " Row(ContentId='6933', sum=305),\n",
       " Row(ContentId='15804', sum=305),\n",
       " Row(ContentId='5259', sum=304),\n",
       " Row(ContentId='12986', sum=304),\n",
       " Row(ContentId='6518', sum=304),\n",
       " Row(ContentId='5830', sum=304),\n",
       " Row(ContentId='4464', sum=303),\n",
       " Row(ContentId='4850', sum=303),\n",
       " Row(ContentId='5097', sum=302),\n",
       " Row(ContentId='9270', sum=302),\n",
       " Row(ContentId='4169', sum=302),\n",
       " Row(ContentId='13840', sum=302),\n",
       " Row(ContentId='5399', sum=302),\n",
       " Row(ContentId='14652', sum=301),\n",
       " Row(ContentId='10198', sum=301),\n",
       " Row(ContentId='4388', sum=301),\n",
       " Row(ContentId='14030', sum=301),\n",
       " Row(ContentId='15914', sum=301),\n",
       " Row(ContentId='13397', sum=301),\n",
       " Row(ContentId='5631', sum=301),\n",
       " Row(ContentId='14058', sum=301),\n",
       " Row(ContentId='12485', sum=301),\n",
       " Row(ContentId='10904', sum=301),\n",
       " Row(ContentId='15262', sum=300),\n",
       " Row(ContentId='11059', sum=300),\n",
       " Row(ContentId='10357', sum=300),\n",
       " Row(ContentId='7296', sum=300),\n",
       " Row(ContentId='17837', sum=300),\n",
       " Row(ContentId='9723', sum=300),\n",
       " Row(ContentId='8563', sum=300),\n",
       " Row(ContentId='11518', sum=300),\n",
       " Row(ContentId='5918', sum=300),\n",
       " Row(ContentId='13259', sum=300),\n",
       " Row(ContentId='8374', sum=300),\n",
       " Row(ContentId='6333', sum=299),\n",
       " Row(ContentId='7796', sum=299),\n",
       " Row(ContentId='11576', sum=299),\n",
       " Row(ContentId='5709', sum=299),\n",
       " Row(ContentId='5876', sum=299),\n",
       " Row(ContentId='8614', sum=298),\n",
       " Row(ContentId='16187', sum=298),\n",
       " Row(ContentId='4103', sum=298),\n",
       " Row(ContentId='12851', sum=298),\n",
       " Row(ContentId='6022', sum=298),\n",
       " Row(ContentId='11058', sum=298),\n",
       " Row(ContentId='8396', sum=298),\n",
       " Row(ContentId='15974', sum=297),\n",
       " Row(ContentId='15350', sum=297),\n",
       " Row(ContentId='7531', sum=297),\n",
       " Row(ContentId='13120', sum=297),\n",
       " Row(ContentId='15069', sum=297),\n",
       " Row(ContentId='16582', sum=297),\n",
       " Row(ContentId='13774', sum=297),\n",
       " Row(ContentId='10463', sum=296),\n",
       " Row(ContentId='9282', sum=296),\n",
       " Row(ContentId='13294', sum=296),\n",
       " Row(ContentId='14935', sum=296),\n",
       " Row(ContentId='8249', sum=296),\n",
       " Row(ContentId='8476', sum=296),\n",
       " Row(ContentId='7982', sum=295),\n",
       " Row(ContentId='17160', sum=295),\n",
       " Row(ContentId='5048', sum=295),\n",
       " Row(ContentId='11165', sum=295),\n",
       " Row(ContentId='4510', sum=295),\n",
       " Row(ContentId='12151', sum=295),\n",
       " Row(ContentId='17834', sum=295),\n",
       " Row(ContentId='10409', sum=295),\n",
       " Row(ContentId='15239', sum=295),\n",
       " Row(ContentId='8218', sum=295),\n",
       " Row(ContentId='14697', sum=294),\n",
       " Row(ContentId='7641', sum=294),\n",
       " Row(ContentId='7144', sum=294),\n",
       " Row(ContentId='14756', sum=294),\n",
       " Row(ContentId='9469', sum=294),\n",
       " Row(ContentId='5420', sum=294),\n",
       " Row(ContentId='17174', sum=294),\n",
       " Row(ContentId='16931', sum=294),\n",
       " Row(ContentId='14815', sum=293),\n",
       " Row(ContentId='5053', sum=292),\n",
       " Row(ContentId='4075', sum=292),\n",
       " Row(ContentId='10755', sum=292),\n",
       " Row(ContentId='14748', sum=292),\n",
       " Row(ContentId='7720', sum=292),\n",
       " Row(ContentId='8712', sum=291),\n",
       " Row(ContentId='15035', sum=291),\n",
       " Row(ContentId='12113', sum=291),\n",
       " Row(ContentId='5052', sum=291),\n",
       " Row(ContentId='14721', sum=291),\n",
       " Row(ContentId='5128', sum=291),\n",
       " Row(ContentId='14789', sum=291),\n",
       " Row(ContentId='13995', sum=291),\n",
       " Row(ContentId='4124', sum=291),\n",
       " Row(ContentId='17331', sum=291),\n",
       " Row(ContentId='9349', sum=291),\n",
       " Row(ContentId='5274', sum=291),\n",
       " Row(ContentId='5427', sum=291),\n",
       " Row(ContentId='10074', sum=290),\n",
       " Row(ContentId='8893', sum=290),\n",
       " Row(ContentId='14258', sum=290),\n",
       " Row(ContentId='13901', sum=290),\n",
       " Row(ContentId='13284', sum=290),\n",
       " Row(ContentId='7256', sum=289),\n",
       " Row(ContentId='4733', sum=289),\n",
       " Row(ContentId='15877', sum=289),\n",
       " Row(ContentId='12009', sum=289),\n",
       " Row(ContentId='5486', sum=289),\n",
       " Row(ContentId='10520', sum=289),\n",
       " Row(ContentId='16449', sum=288),\n",
       " Row(ContentId='11227', sum=287),\n",
       " Row(ContentId='14654', sum=287),\n",
       " Row(ContentId='4511', sum=287),\n",
       " Row(ContentId='14238', sum=287),\n",
       " Row(ContentId='17356', sum=287),\n",
       " Row(ContentId='8348', sum=286),\n",
       " Row(ContentId='4678', sum=286),\n",
       " Row(ContentId='15456', sum=286),\n",
       " Row(ContentId='8422', sum=286),\n",
       " Row(ContentId='10712', sum=286),\n",
       " Row(ContentId='8970', sum=285),\n",
       " Row(ContentId='6303', sum=285),\n",
       " Row(ContentId='12202', sum=285),\n",
       " Row(ContentId='16487', sum=285),\n",
       " Row(ContentId='8777', sum=285),\n",
       " Row(ContentId='8264', sum=285),\n",
       " Row(ContentId='12218', sum=284),\n",
       " Row(ContentId='9884', sum=284),\n",
       " Row(ContentId='14845', sum=284),\n",
       " Row(ContentId='15375', sum=284),\n",
       " Row(ContentId='15741', sum=284),\n",
       " Row(ContentId='8960', sum=284),\n",
       " Row(ContentId='13448', sum=284),\n",
       " Row(ContentId='5516', sum=283),\n",
       " Row(ContentId='15150', sum=282),\n",
       " Row(ContentId='9065', sum=282),\n",
       " Row(ContentId='5302', sum=282),\n",
       " Row(ContentId='10126', sum=282),\n",
       " Row(ContentId='16488', sum=282),\n",
       " Row(ContentId='4792', sum=282),\n",
       " Row(ContentId='4514', sum=282),\n",
       " Row(ContentId='9991', sum=281),\n",
       " Row(ContentId='16057', sum=281),\n",
       " Row(ContentId='6932', sum=281),\n",
       " Row(ContentId='9617', sum=281),\n",
       " Row(ContentId='4848', sum=281),\n",
       " Row(ContentId='17076', sum=281),\n",
       " Row(ContentId='5498', sum=281),\n",
       " Row(ContentId='8790', sum=281),\n",
       " Row(ContentId='5261', sum=280),\n",
       " Row(ContentId='14398', sum=280),\n",
       " Row(ContentId='12243', sum=280),\n",
       " Row(ContentId='7218', sum=280),\n",
       " Row(ContentId='13202', sum=280),\n",
       " Row(ContentId='16740', sum=279),\n",
       " Row(ContentId='12406', sum=279),\n",
       " Row(ContentId='10899', sum=279),\n",
       " Row(ContentId='13755', sum=279),\n",
       " Row(ContentId='4141', sum=279),\n",
       " Row(ContentId='16630', sum=279),\n",
       " Row(ContentId='16351', sum=279),\n",
       " Row(ContentId='6507', sum=279),\n",
       " Row(ContentId='3913', sum=279),\n",
       " Row(ContentId='13669', sum=279),\n",
       " Row(ContentId='12981', sum=279),\n",
       " Row(ContentId='7214', sum=279),\n",
       " Row(ContentId='17333', sum=278),\n",
       " Row(ContentId='6603', sum=278),\n",
       " Row(ContentId='13170', sum=278),\n",
       " Row(ContentId='9425', sum=278),\n",
       " Row(ContentId='12156', sum=278),\n",
       " Row(ContentId='14445', sum=278),\n",
       " Row(ContentId='8238', sum=278),\n",
       " Row(ContentId='15077', sum=277),\n",
       " Row(ContentId='6370', sum=277),\n",
       " Row(ContentId='6268', sum=277),\n",
       " Row(ContentId='9171', sum=277),\n",
       " Row(ContentId='10473', sum=276),\n",
       " Row(ContentId='15273', sum=276),\n",
       " Row(ContentId='13989', sum=276),\n",
       " Row(ContentId='9271', sum=276),\n",
       " Row(ContentId='17833', sum=276),\n",
       " Row(ContentId='4353', sum=276),\n",
       " Row(ContentId='11551', sum=276),\n",
       " Row(ContentId='17588', sum=275),\n",
       " Row(ContentId='3991', sum=275),\n",
       " Row(ContentId='11820', sum=275),\n",
       " Row(ContentId='7019', sum=275),\n",
       " Row(ContentId='11580', sum=275),\n",
       " Row(ContentId='6354', sum=274),\n",
       " Row(ContentId='15647', sum=274),\n",
       " Row(ContentId='7343', sum=274),\n",
       " Row(ContentId='13237', sum=274),\n",
       " Row(ContentId='14281', sum=274),\n",
       " Row(ContentId='9411', sum=273),\n",
       " Row(ContentId='8936', sum=273),\n",
       " Row(ContentId='13877', sum=273),\n",
       " Row(ContentId='7372', sum=273),\n",
       " Row(ContentId='13273', sum=273),\n",
       " Row(ContentId='11195', sum=273),\n",
       " Row(ContentId='13018', sum=273),\n",
       " Row(ContentId='8316', sum=273),\n",
       " Row(ContentId='8158', sum=273),\n",
       " Row(ContentId='4205', sum=272),\n",
       " Row(ContentId='8490', sum=272),\n",
       " Row(ContentId='11772', sum=272),\n",
       " Row(ContentId='15661', sum=272),\n",
       " Row(ContentId='15097', sum=272),\n",
       " Row(ContentId='4465', sum=272),\n",
       " Row(ContentId='13582', sum=271),\n",
       " Row(ContentId='14453', sum=271),\n",
       " Row(ContentId='14819', sum=271),\n",
       " Row(ContentId='17560', sum=271),\n",
       " Row(ContentId='14388', sum=271),\n",
       " Row(ContentId='13222', sum=271),\n",
       " Row(ContentId='7691', sum=271),\n",
       " Row(ContentId='11290', sum=271),\n",
       " Row(ContentId='15274', sum=271),\n",
       " Row(ContentId='8772', sum=270),\n",
       " Row(ContentId='15419', sum=270),\n",
       " Row(ContentId='16827', sum=270),\n",
       " Row(ContentId='4556', sum=270),\n",
       " Row(ContentId='17864', sum=269),\n",
       " Row(ContentId='12064', sum=269),\n",
       " Row(ContentId='15770', sum=269),\n",
       " Row(ContentId='4675', sum=269),\n",
       " Row(ContentId='17809', sum=269),\n",
       " Row(ContentId='8192', sum=269),\n",
       " Row(ContentId='8377', sum=269),\n",
       " Row(ContentId='10950', sum=269),\n",
       " Row(ContentId='5879', sum=269),\n",
       " Row(ContentId='4747', sum=268),\n",
       " Row(ContentId='6676', sum=268),\n",
       " Row(ContentId='8087', sum=268),\n",
       " Row(ContentId='3922', sum=268),\n",
       " Row(ContentId='16039', sum=268),\n",
       " Row(ContentId='11546', sum=268),\n",
       " Row(ContentId='5421', sum=268),\n",
       " Row(ContentId='13644', sum=268),\n",
       " Row(ContentId='4203', sum=268),\n",
       " Row(ContentId='13304', sum=268),\n",
       " Row(ContentId='16489', sum=268),\n",
       " Row(ContentId='8923', sum=267),\n",
       " Row(ContentId='4435', sum=267),\n",
       " Row(ContentId='15458', sum=267),\n",
       " Row(ContentId='6093', sum=267),\n",
       " Row(ContentId='15286', sum=267),\n",
       " Row(ContentId='17652', sum=267),\n",
       " Row(ContentId='8833', sum=267),\n",
       " Row(ContentId='5116', sum=267),\n",
       " Row(ContentId='6010', sum=267),\n",
       " Row(ContentId='16806', sum=267),\n",
       " Row(ContentId='7735', sum=267),\n",
       " Row(ContentId='4585', sum=267),\n",
       " Row(ContentId='4472', sum=267),\n",
       " Row(ContentId='5784', sum=267),\n",
       " Row(ContentId='14712', sum=267),\n",
       " Row(ContentId='14100', sum=266),\n",
       " Row(ContentId='14239', sum=266),\n",
       " Row(ContentId='10580', sum=266),\n",
       " Row(ContentId='10876', sum=266),\n",
       " Row(ContentId='4817', sum=266),\n",
       " Row(ContentId='6823', sum=265),\n",
       " Row(ContentId='5484', sum=265),\n",
       " Row(ContentId='12472', sum=265),\n",
       " Row(ContentId='16393', sum=265),\n",
       " Row(ContentId='14090', sum=264),\n",
       " Row(ContentId='5453', sum=264),\n",
       " Row(ContentId='15999', sum=264),\n",
       " Row(ContentId='13210', sum=264),\n",
       " Row(ContentId='17292', sum=264),\n",
       " Row(ContentId='17780', sum=263),\n",
       " Row(ContentId='8181', sum=263),\n",
       " Row(ContentId='13007', sum=263),\n",
       " Row(ContentId='12781', sum=263),\n",
       " Row(ContentId='13113', sum=263),\n",
       " Row(ContentId='13087', sum=263),\n",
       " Row(ContentId='10460', sum=263),\n",
       " Row(ContentId='6486', sum=263),\n",
       " Row(ContentId='17371', sum=262),\n",
       " Row(ContentId='10624', sum=262),\n",
       " Row(ContentId='7635', sum=262),\n",
       " Row(ContentId='7590', sum=262),\n",
       " Row(ContentId='7949', sum=262),\n",
       " Row(ContentId='10971', sum=262),\n",
       " Row(ContentId='16844', sum=262),\n",
       " Row(ContentId='4919', sum=262),\n",
       " Row(ContentId='10831', sum=262),\n",
       " Row(ContentId='14481', sum=262),\n",
       " Row(ContentId='4261', sum=261),\n",
       " Row(ContentId='13314', sum=261),\n",
       " Row(ContentId='13893', sum=261),\n",
       " Row(ContentId='11236', sum=261),\n",
       " Row(ContentId='15627', sum=261),\n",
       " Row(ContentId='16052', sum=261),\n",
       " Row(ContentId='11633', sum=261),\n",
       " Row(ContentId='11947', sum=261),\n",
       " Row(ContentId='6967', sum=261),\n",
       " Row(ContentId='3920', sum=261),\n",
       " Row(ContentId='12891', sum=261),\n",
       " Row(ContentId='16512', sum=261),\n",
       " Row(ContentId='6866', sum=261),\n",
       " Row(ContentId='15813', sum=261),\n",
       " Row(ContentId='16914', sum=260),\n",
       " Row(ContentId='11941', sum=260),\n",
       " Row(ContentId='10726', sum=260),\n",
       " Row(ContentId='16545', sum=260),\n",
       " Row(ContentId='8055', sum=260),\n",
       " Row(ContentId='7890', sum=260),\n",
       " Row(ContentId='11759', sum=260),\n",
       " Row(ContentId='5049', sum=260),\n",
       " Row(ContentId='6713', sum=260),\n",
       " Row(ContentId='6397', sum=260),\n",
       " Row(ContentId='9853', sum=260),\n",
       " Row(ContentId='11694', sum=260),\n",
       " Row(ContentId='15679', sum=260),\n",
       " Row(ContentId='9737', sum=260),\n",
       " Row(ContentId='8698', sum=260),\n",
       " Row(ContentId='11131', sum=259),\n",
       " Row(ContentId='7092', sum=259),\n",
       " Row(ContentId='17014', sum=259),\n",
       " Row(ContentId='17191', sum=259),\n",
       " Row(ContentId='11852', sum=259),\n",
       " Row(ContentId='4557', sum=259),\n",
       " Row(ContentId='15037', sum=259),\n",
       " Row(ContentId='4218', sum=259),\n",
       " Row(ContentId='14119', sum=259),\n",
       " Row(ContentId='17021', sum=259),\n",
       " Row(ContentId='13290', sum=259),\n",
       " Row(ContentId='6657', sum=258),\n",
       " Row(ContentId='8904', sum=258),\n",
       " Row(ContentId='16294', sum=258),\n",
       " Row(ContentId='6892', sum=258),\n",
       " Row(ContentId='5443', sum=258),\n",
       " Row(ContentId='10662', sum=258),\n",
       " Row(ContentId='8446', sum=258),\n",
       " Row(ContentId='8714', sum=258),\n",
       " Row(ContentId='15090', sum=258),\n",
       " Row(ContentId='13691', sum=258),\n",
       " Row(ContentId='8213', sum=258),\n",
       " Row(ContentId='6335', sum=258),\n",
       " Row(ContentId='12650', sum=258),\n",
       " Row(ContentId='9941', sum=257),\n",
       " Row(ContentId='12729', sum=257),\n",
       " Row(ContentId='6854', sum=257),\n",
       " Row(ContentId='16617', sum=257),\n",
       " Row(ContentId='9107', sum=257),\n",
       " Row(ContentId='5236', sum=257),\n",
       " Row(ContentId='3947', sum=257),\n",
       " Row(ContentId='11911', sum=257),\n",
       " Row(ContentId='15386', sum=257),\n",
       " Row(ContentId='10587', sum=256),\n",
       " Row(ContentId='14843', sum=256),\n",
       " Row(ContentId='11364', sum=256),\n",
       " Row(ContentId='15605', sum=256),\n",
       " Row(ContentId='17283', sum=256),\n",
       " Row(ContentId='4650', sum=256),\n",
       " Row(ContentId='15843', sum=256),\n",
       " Row(ContentId='11190', sum=256),\n",
       " Row(ContentId='6207', sum=256),\n",
       " Row(ContentId='14695', sum=256),\n",
       " Row(ContentId='12388', sum=255),\n",
       " Row(ContentId='16468', sum=255),\n",
       " Row(ContentId='12191', sum=255),\n",
       " Row(ContentId='9375', sum=255),\n",
       " Row(ContentId='12925', sum=255),\n",
       " Row(ContentId='11113', sum=255),\n",
       " Row(ContentId='5909', sum=255),\n",
       " Row(ContentId='15241', sum=255),\n",
       " Row(ContentId='7411', sum=255),\n",
       " Row(ContentId='9161', sum=254),\n",
       " Row(ContentId='9009', sum=254),\n",
       " Row(ContentId='4996', sum=254),\n",
       " Row(ContentId='13964', sum=254),\n",
       " Row(ContentId='10441', sum=254),\n",
       " Row(ContentId='16975', sum=254),\n",
       " Row(ContentId='13764', sum=254),\n",
       " Row(ContentId='14958', sum=254),\n",
       " Row(ContentId='7069', sum=254),\n",
       " Row(ContentId='16479', sum=254),\n",
       " Row(ContentId='7065', sum=254),\n",
       " Row(ContentId='5736', sum=254),\n",
       " Row(ContentId='5864', sum=254),\n",
       " Row(ContentId='5479', sum=254),\n",
       " Row(ContentId='6576', sum=254),\n",
       " Row(ContentId='9298', sum=253),\n",
       " Row(ContentId='14297', sum=253),\n",
       " Row(ContentId='10805', sum=253),\n",
       " Row(ContentId='16561', sum=253),\n",
       " Row(ContentId='14139', sum=253),\n",
       " Row(ContentId='8216', sum=253),\n",
       " Row(ContentId='14936', sum=253),\n",
       " Row(ContentId='11674', sum=253),\n",
       " Row(ContentId='7998', sum=253),\n",
       " Row(ContentId='7763', sum=253),\n",
       " Row(ContentId='16387', sum=253),\n",
       " Row(ContentId='15171', sum=253),\n",
       " Row(ContentId='10294', sum=253),\n",
       " Row(ContentId='17210', sum=252),\n",
       " Row(ContentId='16028', sum=252),\n",
       " Row(ContentId='10859', sum=252),\n",
       " Row(ContentId='10701', sum=252),\n",
       " Row(ContentId='8062', sum=252),\n",
       " Row(ContentId='5226', sum=252),\n",
       " Row(ContentId='16142', sum=252),\n",
       " Row(ContentId='11811', sum=252),\n",
       " Row(ContentId='13272', sum=252),\n",
       " Row(ContentId='8313', sum=252),\n",
       " Row(ContentId='10355', sum=252),\n",
       " Row(ContentId='15013', sum=251),\n",
       " Row(ContentId='15555', sum=251),\n",
       " Row(ContentId='14207', sum=251),\n",
       " Row(ContentId='5904', sum=251),\n",
       " Row(ContentId='15905', sum=251),\n",
       " Row(ContentId='7187', sum=251),\n",
       " Row(ContentId='7369', sum=250),\n",
       " Row(ContentId='15590', sum=250),\n",
       " Row(ContentId='7857', sum=250),\n",
       " Row(ContentId='8885', sum=250),\n",
       " Row(ContentId='6908', sum=250),\n",
       " Row(ContentId='9947', sum=250),\n",
       " Row(ContentId='7079', sum=250),\n",
       " Row(ContentId='16285', sum=250),\n",
       " Row(ContentId='10470', sum=250),\n",
       " Row(ContentId='16571', sum=250),\n",
       " Row(ContentId='12455', sum=250),\n",
       " Row(ContentId='5108', sum=250),\n",
       " Row(ContentId='7706', sum=250),\n",
       " Row(ContentId='12844', sum=250),\n",
       " Row(ContentId='13057', sum=249),\n",
       " Row(ContentId='16035', sum=249),\n",
       " Row(ContentId='14470', sum=249),\n",
       " Row(ContentId='14659', sum=249),\n",
       " Row(ContentId='16837', sum=249),\n",
       " Row(ContentId='12577', sum=249),\n",
       " Row(ContentId='8327', sum=249),\n",
       " Row(ContentId='16193', sum=249),\n",
       " Row(ContentId='11497', sum=249),\n",
       " Row(ContentId='4526', sum=249),\n",
       " Row(ContentId='6445', sum=249),\n",
       " Row(ContentId='7051', sum=249),\n",
       " Row(ContentId='14583', sum=249),\n",
       " Row(ContentId='4990', sum=249),\n",
       " Row(ContentId='13144', sum=249),\n",
       " Row(ContentId='8114', sum=248),\n",
       " Row(ContentId='12282', sum=248),\n",
       " Row(ContentId='10274', sum=248),\n",
       " Row(ContentId='12732', sum=248),\n",
       " Row(ContentId='7056', sum=248),\n",
       " Row(ContentId='14770', sum=248),\n",
       " Row(ContentId='5634', sum=248),\n",
       " Row(ContentId='6785', sum=248),\n",
       " Row(ContentId='14007', sum=248),\n",
       " Row(ContentId='16495', sum=248),\n",
       " Row(ContentId='14176', sum=248),\n",
       " Row(ContentId='12579', sum=248),\n",
       " Row(ContentId='7575', sum=248),\n",
       " Row(ContentId='9811', sum=248),\n",
       " Row(ContentId='6049', sum=248),\n",
       " Row(ContentId='8024', sum=248),\n",
       " Row(ContentId='17416', sum=247),\n",
       " Row(ContentId='5767', sum=247),\n",
       " Row(ContentId='13367', sum=247),\n",
       " Row(ContentId='9165', sum=247),\n",
       " Row(ContentId='11328', sum=247),\n",
       " Row(ContentId='9956', sum=247),\n",
       " Row(ContentId='4240', sum=247),\n",
       " Row(ContentId='15579', sum=247),\n",
       " Row(ContentId='6918', sum=247),\n",
       " Row(ContentId='12665', sum=247),\n",
       " Row(ContentId='15603', sum=247),\n",
       " Row(ContentId='5674', sum=247),\n",
       " Row(ContentId='8408', sum=247),\n",
       " Row(ContentId='16350', sum=247),\n",
       " Row(ContentId='9670', sum=246),\n",
       " Row(ContentId='7748', sum=246),\n",
       " Row(ContentId='10456', sum=246),\n",
       " Row(ContentId='9460', sum=246),\n",
       " Row(ContentId='14220', sum=246),\n",
       " Row(ContentId='9371', sum=246),\n",
       " Row(ContentId='10621', sum=246),\n",
       " Row(ContentId='16737', sum=246),\n",
       " Row(ContentId='14541', sum=246),\n",
       " Row(ContentId='4591', sum=246),\n",
       " Row(ContentId='4318', sum=246),\n",
       " Row(ContentId='7494', sum=246),\n",
       " Row(ContentId='13418', sum=246),\n",
       " Row(ContentId='11651', sum=246),\n",
       " Row(ContentId='16773', sum=246),\n",
       " Row(ContentId='15092', sum=246),\n",
       " Row(ContentId='6684', sum=246),\n",
       " Row(ContentId='16701', sum=246),\n",
       " Row(ContentId='16528', sum=246),\n",
       " Row(ContentId='15569', sum=245),\n",
       " Row(ContentId='9036', sum=245),\n",
       " Row(ContentId='16771', sum=245),\n",
       " Row(ContentId='7870', sum=245),\n",
       " Row(ContentId='14212', sum=245),\n",
       " Row(ContentId='11939', sum=245),\n",
       " Row(ContentId='5327', sum=245),\n",
       " Row(ContentId='6669', sum=245),\n",
       " Row(ContentId='10145', sum=245),\n",
       " Row(ContentId='5772', sum=245),\n",
       " Row(ContentId='6291', sum=245),\n",
       " Row(ContentId='7410', sum=245),\n",
       " Row(ContentId='14646', sum=245),\n",
       " Row(ContentId='10815', sum=245),\n",
       " Row(ContentId='12604', sum=244),\n",
       " Row(ContentId='4983', sum=244),\n",
       " Row(ContentId='12607', sum=244),\n",
       " Row(ContentId='16413', sum=244),\n",
       " Row(ContentId='7675', sum=244),\n",
       " Row(ContentId='5912', sum=244),\n",
       " Row(ContentId='8634', sum=244),\n",
       " Row(ContentId='3937', sum=244),\n",
       " Row(ContentId='16886', sum=244),\n",
       " Row(ContentId='5127', sum=244),\n",
       " Row(ContentId='7543', sum=244),\n",
       " Row(ContentId='4008', sum=244),\n",
       " Row(ContentId='14171', sum=243),\n",
       " Row(ContentId='7608', sum=243),\n",
       " Row(ContentId='5870', sum=243),\n",
       " Row(ContentId='3978', sum=243),\n",
       " Row(ContentId='13484', sum=243),\n",
       " Row(ContentId='10002', sum=243),\n",
       " Row(ContentId='14775', sum=243),\n",
       " Row(ContentId='14755', sum=243),\n",
       " Row(ContentId='15686', sum=243),\n",
       " Row(ContentId='17711', sum=243),\n",
       " Row(ContentId='10079', sum=243),\n",
       " Row(ContentId='5100', sum=243),\n",
       " Row(ContentId='12042', sum=243),\n",
       " Row(ContentId='14180', sum=242),\n",
       " Row(ContentId='15542', sum=242),\n",
       " Row(ContentId='7310', sum=242),\n",
       " Row(ContentId='12257', sum=242),\n",
       " Row(ContentId='13926', sum=242),\n",
       " Row(ContentId='8067', sum=242),\n",
       " Row(ContentId='12744', sum=242),\n",
       " Row(ContentId='14785', sum=242),\n",
       " Row(ContentId='10006', sum=242),\n",
       " Row(ContentId='13625', sum=242),\n",
       " Row(ContentId='7466', sum=242),\n",
       " Row(ContentId='7869', sum=242),\n",
       " Row(ContentId='5971', sum=242),\n",
       " Row(ContentId='14131', sum=242),\n",
       " Row(ContentId='8144', sum=242),\n",
       " Row(ContentId='9753', sum=241),\n",
       " Row(ContentId='6586', sum=241),\n",
       " Row(ContentId='9719', sum=241),\n",
       " Row(ContentId='12609', sum=241),\n",
       " Row(ContentId='13541', sum=241),\n",
       " Row(ContentId='4134', sum=241),\n",
       " Row(ContentId='7288', sum=241),\n",
       " Row(ContentId='15800', sum=241),\n",
       " Row(ContentId='14730', sum=241),\n",
       " Row(ContentId='12722', sum=241),\n",
       " Row(ContentId='5229', sum=241),\n",
       " Row(ContentId='7278', sum=241),\n",
       " Row(ContentId='7349', sum=241),\n",
       " Row(ContentId='4427', sum=241),\n",
       " Row(ContentId='4804', sum=241),\n",
       " Row(ContentId='11974', sum=241),\n",
       " Row(ContentId='13918', sum=241),\n",
       " Row(ContentId='15982', sum=241),\n",
       " Row(ContentId='9450', sum=241),\n",
       " Row(ContentId='6633', sum=240),\n",
       " Row(ContentId='6188', sum=240),\n",
       " Row(ContentId='11738', sum=240),\n",
       " Row(ContentId='6244', sum=240),\n",
       " Row(ContentId='8813', sum=240),\n",
       " Row(ContentId='9053', sum=240),\n",
       " Row(ContentId='16338', sum=240),\n",
       " Row(ContentId='10670', sum=240),\n",
       " Row(ContentId='4003', sum=240),\n",
       " Row(ContentId='16820', sum=240),\n",
       " Row(ContentId='15326', sum=239),\n",
       " Row(ContentId='15586', sum=239),\n",
       " Row(ContentId='14002', sum=239),\n",
       " Row(ContentId='10377', sum=239),\n",
       " Row(ContentId='14831', sum=239),\n",
       " Row(ContentId='17867', sum=239),\n",
       " Row(ContentId='8927', sum=239),\n",
       " Row(ContentId='9653', sum=239),\n",
       " Row(ContentId='8353', sum=239),\n",
       " Row(ContentId='15266', sum=239),\n",
       " Row(ContentId='10618', sum=239),\n",
       " Row(ContentId='7432', sum=239),\n",
       " Row(ContentId='8630', sum=239),\n",
       " Row(ContentId='6062', sum=239),\n",
       " Row(ContentId='16749', sum=239),\n",
       " Row(ContentId='6624', sum=239),\n",
       " Row(ContentId='16768', sum=239),\n",
       " Row(ContentId='11079', sum=238),\n",
       " Row(ContentId='15595', sum=238),\n",
       " Row(ContentId='6563', sum=238),\n",
       " Row(ContentId='15985', sum=238),\n",
       " Row(ContentId='8080', sum=238),\n",
       " Row(ContentId='14922', sum=238),\n",
       " Row(ContentId='14435', sum=238),\n",
       " Row(ContentId='11653', sum=238),\n",
       " Row(ContentId='11284', sum=238),\n",
       " Row(ContentId='13147', sum=238),\n",
       " Row(ContentId='14534', sum=238),\n",
       " Row(ContentId='7153', sum=238),\n",
       " Row(ContentId='11593', sum=238),\n",
       " Row(ContentId='17681', sum=237),\n",
       " Row(ContentId='16361', sum=237),\n",
       " Row(ContentId='7719', sum=237),\n",
       " Row(ContentId='16395', sum=237),\n",
       " Row(ContentId='13865', sum=237),\n",
       " Row(ContentId='12636', sum=237),\n",
       " Row(ContentId='9816', sum=237),\n",
       " Row(ContentId='12220', sum=237),\n",
       " Row(ContentId='15020', sum=237),\n",
       " Row(ContentId='6193', sum=237),\n",
       " Row(ContentId='5939', sum=237),\n",
       " Row(ContentId='9226', sum=237),\n",
       " Row(ContentId='11281', sum=237),\n",
       " Row(ContentId='15820', sum=237),\n",
       " Row(ContentId='13871', sum=237),\n",
       " Row(ContentId='15451', sum=237),\n",
       " Row(ContentId='10304', sum=237),\n",
       " Row(ContentId='8156', sum=237),\n",
       " Row(ContentId='15104', sum=237),\n",
       " Row(ContentId='13148', sum=237),\n",
       " Row(ContentId='16578', sum=236),\n",
       " Row(ContentId='15706', sum=236),\n",
       " Row(ContentId='10609', sum=236),\n",
       " Row(ContentId='6544', sum=236),\n",
       " Row(ContentId='6070', sum=236),\n",
       " Row(ContentId='11307', sum=236),\n",
       " Row(ContentId='5190', sum=236),\n",
       " Row(ContentId='4955', sum=236),\n",
       " Row(ContentId='16877', sum=236),\n",
       " Row(ContentId='9951', sum=236),\n",
       " Row(ContentId='14447', sum=236),\n",
       " Row(ContentId='12888', sum=236),\n",
       " Row(ContentId='9712', sum=236),\n",
       " Row(ContentId='17011', sum=236),\n",
       " Row(ContentId='12735', sum=236),\n",
       " Row(ContentId='13109', sum=236),\n",
       " Row(ContentId='8876', sum=235),\n",
       " Row(ContentId='17439', sum=235),\n",
       " Row(ContentId='7034', sum=235),\n",
       " Row(ContentId='11709', sum=235),\n",
       " Row(ContentId='5682', sum=235),\n",
       " Row(ContentId='9456', sum=235),\n",
       " Row(ContentId='8340', sum=235),\n",
       " Row(ContentId='10718', sum=235),\n",
       " Row(ContentId='16883', sum=235),\n",
       " Row(ContentId='13045', sum=235),\n",
       " Row(ContentId='17182', sum=235),\n",
       " Row(ContentId='16292', sum=235),\n",
       " Row(ContentId='7556', sum=235),\n",
       " Row(ContentId='12469', sum=235),\n",
       " Row(ContentId='5894', sum=235),\n",
       " Row(ContentId='7757', sum=235),\n",
       " Row(ContentId='17171', sum=235),\n",
       " Row(ContentId='12049', sum=235),\n",
       " Row(ContentId='11909', sum=234),\n",
       " Row(ContentId='11346', sum=234),\n",
       " Row(ContentId='5982', sum=234),\n",
       " Row(ContentId='12582', sum=234),\n",
       " Row(ContentId='13748', sum=234),\n",
       " Row(ContentId='11435', sum=234),\n",
       " Row(ContentId='9374', sum=234),\n",
       " Row(ContentId='14666', sum=234),\n",
       " Row(ContentId='17683', sum=234),\n",
       " Row(ContentId='7928', sum=234),\n",
       " Row(ContentId='4182', sum=234),\n",
       " Row(ContentId='10051', sum=234),\n",
       " Row(ContentId='14300', sum=234),\n",
       " Row(ContentId='6029', sum=234),\n",
       " Row(ContentId='17720', sum=234),\n",
       " Row(ContentId='6882', sum=234),\n",
       " Row(ContentId='5125', sum=234),\n",
       " Row(ContentId='6757', sum=234),\n",
       " Row(ContentId='6738', sum=234),\n",
       " Row(ContentId='10766', sum=233),\n",
       " Row(ContentId='4830', sum=233),\n",
       " Row(ContentId='5402', sum=233),\n",
       " Row(ContentId='15659', sum=233),\n",
       " Row(ContentId='7707', sum=233),\n",
       " Row(ContentId='14581', sum=233),\n",
       " Row(ContentId='12868', sum=233),\n",
       " Row(ContentId='4009', sum=233),\n",
       " Row(ContentId='8163', sum=233),\n",
       " Row(ContentId='4600', sum=233),\n",
       " Row(ContentId='4444', sum=233),\n",
       " Row(ContentId='4754', sum=232),\n",
       " Row(ContentId='9215', sum=232),\n",
       " Row(ContentId='16473', sum=232),\n",
       " Row(ContentId='12487', sum=232),\n",
       " Row(ContentId='15222', sum=232),\n",
       " Row(ContentId='5431', sum=232),\n",
       " Row(ContentId='15695', sum=232),\n",
       " Row(ContentId='17417', sum=232),\n",
       " Row(ContentId='15912', sum=232),\n",
       " Row(ContentId='4901', sum=232),\n",
       " Row(ContentId='14354', sum=232),\n",
       " Row(ContentId='17797', sum=232),\n",
       " Row(ContentId='17037', sum=232),\n",
       " Row(ContentId='13358', sum=232),\n",
       " Row(ContentId='13191', sum=232),\n",
       " Row(ContentId='12930', sum=232),\n",
       " Row(ContentId='15958', sum=232),\n",
       " Row(ContentId='11396', sum=232),\n",
       " Row(ContentId='6037', sum=232),\n",
       " Row(ContentId='11010', sum=232),\n",
       " Row(ContentId='12715', sum=232),\n",
       " Row(ContentId='4653', sum=232),\n",
       " Row(ContentId='16979', sum=232),\n",
       " Row(ContentId='14506', sum=231),\n",
       " Row(ContentId='15249', sum=231),\n",
       " Row(ContentId='10202', sum=231),\n",
       " Row(ContentId='13251', sum=231),\n",
       " Row(ContentId='6411', sum=231),\n",
       " Row(ContentId='11837', sum=231),\n",
       " Row(ContentId='16453', sum=231),\n",
       " Row(ContentId='14893', sum=231),\n",
       " Row(ContentId='16881', sum=231),\n",
       " Row(ContentId='14101', sum=231),\n",
       " Row(ContentId='5292', sum=231),\n",
       " Row(ContentId='4047', sum=231),\n",
       " Row(ContentId='8798', sum=231),\n",
       " Row(ContentId='10242', sum=231),\n",
       " Row(ContentId='13310', sum=231),\n",
       " Row(ContentId='4375', sum=231),\n",
       " Row(ContentId='9663', sum=231),\n",
       " Row(ContentId='15660', sum=231),\n",
       " Row(ContentId='10689', sum=231),\n",
       " Row(ContentId='11983', sum=231),\n",
       " Row(ContentId='8518', sum=231),\n",
       " Row(ContentId='15405', sum=231),\n",
       " Row(ContentId='17374', sum=231),\n",
       " Row(ContentId='12560', sum=231),\n",
       " Row(ContentId='9266', sum=230),\n",
       " Row(ContentId='6796', sum=230),\n",
       " Row(ContentId='14458', sum=230),\n",
       " Row(ContentId='6393', sum=230),\n",
       " Row(ContentId='15588', sum=230),\n",
       " Row(ContentId='9570', sum=230),\n",
       " Row(ContentId='4772', sum=230),\n",
       " Row(ContentId='16531', sum=230),\n",
       " Row(ContentId='16421', sum=230),\n",
       " Row(ContentId='7906', sum=230),\n",
       " Row(ContentId='13231', sum=230),\n",
       " Row(ContentId='15684', sum=230),\n",
       " Row(ContentId='8191', sum=230),\n",
       " Row(ContentId='7875', sum=230),\n",
       " Row(ContentId='4175', sum=230),\n",
       " Row(ContentId='4327', sum=230),\n",
       " Row(ContentId='16875', sum=230),\n",
       " Row(ContentId='11662', sum=230),\n",
       " Row(ContentId='3955', sum=230),\n",
       " Row(ContentId='16030', sum=230),\n",
       " Row(ContentId='6650', sum=230),\n",
       " Row(ContentId='9323', sum=230),\n",
       " Row(ContentId='16148', sum=230),\n",
       " Row(ContentId='12434', sum=230),\n",
       " Row(ContentId='12520', sum=230),\n",
       " Row(ContentId='9782', sum=230),\n",
       " Row(ContentId='7300', sum=230),\n",
       " Row(ContentId='11385', sum=230),\n",
       " Row(ContentId='16324', sum=230),\n",
       " Row(ContentId='4761', sum=230),\n",
       " Row(ContentId='17351', sum=230),\n",
       " Row(ContentId='5191', sum=229),\n",
       " Row(ContentId='5900', sum=229),\n",
       " Row(ContentId='5619', sum=229),\n",
       " Row(ContentId='13021', sum=229),\n",
       " Row(ContentId='9575', sum=229),\n",
       " Row(ContentId='4621', sum=229),\n",
       " Row(ContentId='9115', sum=229),\n",
       " Row(ContentId='14754', sum=229),\n",
       " Row(ContentId='6437', sum=229),\n",
       " Row(ContentId='6061', sum=229),\n",
       " Row(ContentId='5906', sum=229),\n",
       " Row(ContentId='9461', sum=229),\n",
       " Row(ContentId='14761', sum=229),\n",
       " Row(ContentId='8104', sum=229),\n",
       " Row(ContentId='11517', sum=229),\n",
       " Row(ContentId='4879', sum=229),\n",
       " Row(ContentId='6952', sum=229),\n",
       " Row(ContentId='4080', sum=229),\n",
       " Row(ContentId='14740', sum=229),\n",
       " Row(ContentId='4272', sum=229),\n",
       " Row(ContentId='11624', sum=228),\n",
       " Row(ContentId='11008', sum=228),\n",
       " Row(ContentId='6504', sum=228),\n",
       " Row(ContentId='8415', sum=228),\n",
       " Row(ContentId='4404', sum=228),\n",
       " Row(ContentId='5287', sum=228),\n",
       " Row(ContentId='8206', sum=228),\n",
       " Row(ContentId='17261', sum=228),\n",
       " Row(ContentId='9498', sum=228),\n",
       " Row(ContentId='14563', sum=228),\n",
       " Row(ContentId='8166', sum=228),\n",
       " Row(ContentId='14592', sum=228),\n",
       " Row(ContentId='15637', sum=228),\n",
       " Row(ContentId='15592', sum=228),\n",
       " Row(ContentId='8863', sum=228),\n",
       " Row(ContentId='14365', sum=228),\n",
       " Row(ContentId='4554', sum=228),\n",
       " Row(ContentId='11658', sum=228),\n",
       " Row(ContentId='6898', sum=228),\n",
       " Row(ContentId='16386', sum=228),\n",
       " Row(ContentId='10800', sum=228),\n",
       " Row(ContentId='12446', sum=228),\n",
       " Row(ContentId='13058', sum=228),\n",
       " Row(ContentId='9558', sum=228),\n",
       " Row(ContentId='7687', sum=228),\n",
       " Row(ContentId='16177', sum=227),\n",
       " Row(ContentId='4851', sum=227),\n",
       " Row(ContentId='14793', sum=227),\n",
       " Row(ContentId='6920', sum=227),\n",
       " Row(ContentId='9767', sum=227),\n",
       " Row(ContentId='17578', sum=227),\n",
       " Row(ContentId='9126', sum=227),\n",
       " Row(ContentId='14604', sum=227),\n",
       " Row(ContentId='8611', sum=227),\n",
       " Row(ContentId='14117', sum=227),\n",
       " Row(ContentId='6889', sum=227),\n",
       " Row(ContentId='7422', sum=227),\n",
       " Row(ContentId='8732', sum=227),\n",
       " Row(ContentId='8555', sum=227),\n",
       " Row(ContentId='4262', sum=227),\n",
       " Row(ContentId='16950', sum=227),\n",
       " Row(ContentId='4035', sum=227),\n",
       " Row(ContentId='5405', sum=227),\n",
       " Row(ContentId='16744', sum=227),\n",
       " Row(ContentId='16799', sum=227),\n",
       " Row(ContentId='11422', sum=227),\n",
       " Row(ContentId='13846', sum=227),\n",
       " Row(ContentId='10434', sum=227),\n",
       " Row(ContentId='14196', sum=227),\n",
       " Row(ContentId='13319', sum=227),\n",
       " Row(ContentId='16033', sum=227),\n",
       " Row(ContentId='15014', sum=227),\n",
       " Row(ContentId='14328', sum=226),\n",
       " Row(ContentId='9573', sum=226),\n",
       " Row(ContentId='14979', sum=226),\n",
       " Row(ContentId='10204', sum=226),\n",
       " Row(ContentId='13682', sum=226),\n",
       " Row(ContentId='13171', sum=226),\n",
       " Row(ContentId='13227', sum=226),\n",
       " Row(ContentId='11040', sum=226),\n",
       " Row(ContentId='11800', sum=226),\n",
       " Row(ContentId='16542', sum=226),\n",
       " Row(ContentId='14423', sum=226),\n",
       " Row(ContentId='7550', sum=226),\n",
       " Row(ContentId='11039', sum=226),\n",
       " Row(ContentId='16608', sum=226),\n",
       " Row(ContentId='7292', sum=226),\n",
       " Row(ContentId='13348', sum=226),\n",
       " Row(ContentId='8957', sum=226)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(user_play_rdd, ['Id', 'UserId', 'ContentId', 'Duration'])\n",
    "df = df.withColumn(\"Duration\", df[\"Duration\"].cast(IntegerType()))\n",
    "df = df.groupBy('ContentId').sum('Duration').withColumnRenamed('sum(Duration)', 'sum')\n",
    "\n",
    "# 视频被播放的时间长度\n",
    "df.sort('sum', ascending=False).take(1000)\n",
    "# 视频被播放的次数\n",
    "# df.groupBy('ContentId').count().take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "02-intro-spark.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
