{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cardiac-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了能够让 python 找到 pyspark，使用 findspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# 为了使用 RDDs，创建 SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "# 创建 SparkConf 和 SparkSession\n",
    "conf=SparkConf()\\\n",
    "        .setMaster('local[*]')\\\n",
    "        .setAppName(\"~\")\\\n",
    "        .setExecutorEnv(\"spark.executor.memory\",\"2g\")\\\n",
    "        .setExecutorEnv(\"spark.driver.memory\",\"2g\")\n",
    "\n",
    "spark=SparkSession.builder\\\n",
    "        .config(conf=conf)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# 获取 SparkContext\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-infection",
   "metadata": {},
   "source": [
    "# Data aggregations on RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-estonia",
   "metadata": {},
   "source": [
    "We can aggregate RDD data in Spark by using three different actions: `reduce`, `fold`, and `aggregate`. The last one is the more general one and someway includes the first two.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-buffalo",
   "metadata": {},
   "source": [
    "## Getting the data and creating the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-torture",
   "metadata": {},
   "source": [
    "As we did in our first notebook, we will use the reduced dataset (10 percent) provided for the [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html), containing nearly half million nework interactions. The file is provided as a Gzip file that we will download locally.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "first-antibody",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "f = urllib.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \"kddcup.data_10_percent.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tender-translation",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data_file = \"/kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-producer",
   "metadata": {},
   "source": [
    "## Inspecting interaction duration by tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-distribution",
   "metadata": {},
   "source": [
    "Both `fold` and `reduce` take a function as an argument that is applied to two elements of the RDD. The `fold` action differs from `reduce` in that it gets and additional initial *zero value* to be used for the initial call. This value should be the identity element for the function provided.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-hygiene",
   "metadata": {},
   "source": [
    "As an example, imagine we want to know the total duration of our interactions for normal and attack interactions. We can use `reduce` as follows.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "worst-force",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# parse data\n",
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "\n",
    "# separate into different RDDs\n",
    "normal_csv_data = csv_data.filter(lambda x: x[41]==\"normal.\")\n",
    "attack_csv_data = csv_data.filter(lambda x: x[41]!=\"attack.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-siemens",
   "metadata": {},
   "source": [
    "The function that we pass to `reduce` gets and returns elements of the same type of the RDD. If we want to sum durations we need to extract that element into a new RDD.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "psychological-concord",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "normal_duration_data = normal_csv_data.map(lambda x: int(x[0]))\n",
    "attack_duration_data = attack_csv_data.map(lambda x: int(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "surprising-beginning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_duration_data.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-iceland",
   "metadata": {},
   "source": [
    "Now we can reduce these new RDDs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "centered-consciousness",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duration for 'normal' interactions is 21075991\n",
      "Total duration for 'attack' interactions is 23702783\n"
     ]
    }
   ],
   "source": [
    "total_normal_duration = normal_duration_data.reduce(lambda x, y: x + y)\n",
    "total_attack_duration = attack_duration_data.reduce(lambda x, y: x + y)\n",
    "\n",
    "print(\"Total duration for 'normal' interactions is {}\".\\\n",
    "    format(total_normal_duration))\n",
    "print(\"Total duration for 'attack' interactions is {}\".\\\n",
    "    format(total_attack_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-transport",
   "metadata": {},
   "source": [
    "We can go further and use counts to calculate duration means.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "clean-guard",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean duration for 'normal' interactions is 216.657\n",
      "Mean duration for 'attack' interactions is 6.621\n"
     ]
    }
   ],
   "source": [
    "normal_count = normal_duration_data.count()\n",
    "attack_count = attack_duration_data.count()\n",
    "\n",
    "print(\"Mean duration for 'normal' interactions is {}\".\\\n",
    "    format(round(total_normal_duration/float(normal_count),3)))\n",
    "print(\"Mean duration for 'attack' interactions is {}\".\\\n",
    "    format(round(total_attack_duration/float(attack_count),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-trading",
   "metadata": {},
   "source": [
    "We have a first (and too simplistic) approach to identify attack interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-charger",
   "metadata": {},
   "source": [
    "## A better way, using `aggregate`  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-gibson",
   "metadata": {},
   "source": [
    "The `aggregate` action frees us from the constraint of having the return be the same type as the RDD we are working on. Like with `fold`, we supply an initial zero value of the type we want to return. Then we provide two functions. The first one is used to combine the elements from our RDD with the accumulator. The second function is needed to merge two accumulators. Let's see it in action calculating the mean we did before.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "spiritual-colony",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean duration for 'normal' interactions is 216.657\n"
     ]
    }
   ],
   "source": [
    "normal_sum_count = normal_duration_data.aggregate(\n",
    "    (0,0), # the initial value\n",
    "    (lambda acc, value: (acc[0] + value, acc[1] + 1)), # combine value with acc\n",
    "    (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])) # combine accumulators\n",
    ")\n",
    "\n",
    "print(\"Mean duration for 'normal' interactions is {}\".\\\n",
    "    format(round(normal_sum_count[0]/float(normal_sum_count[1]),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-stockholm",
   "metadata": {},
   "source": [
    "In the previous aggregation, the accumulator first element keeps the total sum, while the second element keeps the count. Combining an accumulator with an RDD element consists in summing up the value and incrementing the count. Combining two accumulators requires just a pairwise sum.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-psychology",
   "metadata": {},
   "source": [
    "We can do the same with attack type interactions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "noted-terror",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean duration for 'attack' interactions is 6.621\n"
     ]
    }
   ],
   "source": [
    "attack_sum_count = attack_duration_data.aggregate(\n",
    "    (0,0), # the initial value\n",
    "    (lambda acc, value: (acc[0] + value, acc[1] + 1)), # combine value with acc\n",
    "    (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])) # combine accumulators\n",
    ")\n",
    "\n",
    "print(\"Mean duration for 'attack' interactions is {}\".\\\n",
    "    format(round(attack_sum_count[0]/float(attack_sum_count[1]),3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
